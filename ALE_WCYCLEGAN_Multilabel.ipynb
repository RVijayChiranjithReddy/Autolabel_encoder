{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALE_WCYCLEGAN_Multilabel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMn+6pnmZ3Y3tD09NUef8MJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RVijayChiranjithReddy/Autolabel_encoder/blob/main/ALE_WCYCLEGAN_Multilabel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_59c0-Kvj2z",
        "outputId": "378cbd1c-b7b7-42ed-d178-a75e299ce686"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/4b/e893d194e626c24b3df2253066aa418f46a432fdb68250cde14bf9bb0700/tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679kB)\n",
            "\r\u001b[K     |▌                               | 10kB 23.3MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 27.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 17.8MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 15.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 8.3MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 143kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 174kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 204kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 235kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 266kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 286kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 307kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 327kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 348kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 358kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 368kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 378kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 389kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 399kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 409kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 419kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 430kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 440kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 450kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 460kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 471kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 481kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 491kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 501kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 512kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 522kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 532kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 542kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 552kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 563kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 573kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 583kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 593kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 604kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 614kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 624kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 634kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 645kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 655kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 665kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 675kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 686kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze4ye-Dund5Y",
        "outputId": "75337965-5a6b-4298-cd10-8c90a4329532"
      },
      "source": [
        "\n",
        "from keras.layers import *\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.activations import *\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.applications import *\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential,Model\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow_addons as tfa\n",
        "import glob\n",
        "from random import randint, shuffle\n",
        "%matplotlib inline\n",
        "K.set_learning_phase(1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:400: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0OSQiHInian",
        "outputId": "630b2758-f8bb-4735-dc24-2f0a46efba70"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auh6WiqPnkKJ",
        "outputId": "cf8bddbd-7ade-4e90-c5fa-74e3366f8697"
      },
      "source": [
        "import pandas\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "df = pandas.read_csv('/content/styles.csv')\n",
        "df.fillna('DK', inplace = True) \n",
        "data = {}\n",
        "for i in range(44446):\n",
        "  tags = [str(x) for x in df.loc[i].values[1:9] if type(x) is str]\n",
        "  #desc = df.loc[i].values[9]\n",
        "  #if type(desc) is not float:\n",
        "    #for de in desc.split():\n",
        "     #if de.isalpha() and de not in tags:\n",
        "        #tags.append(de)\n",
        "  data[str(df.loc[i].values[0])] = ':'.join(tags)\n",
        "vocab = []\n",
        "for k,v in data.items():\n",
        "  temp = list(set(v.split(':')))\n",
        "  vocab = vocab + temp\n",
        "  vocab = list(set(vocab))\n",
        "print(len(vocab))\n",
        "#from tensorflow.keras.preprocessing.text import one_hot\n",
        "#for k,d in data.items():\n",
        "  #data_encoded[k] = one_hot(' '.join(d),len(vocab))\n",
        "tokenizer1 = Tokenizer(num_words=25)\n",
        "tokenizer2 = Tokenizer(num_words=25)\n",
        "tokenizer3 = Tokenizer(num_words=54)\n",
        "tokenizer4 = Tokenizer(num_words=173)\n",
        "tokenizer5 = Tokenizer(num_words=48)\n",
        "tokenizer6 = Tokenizer(num_words=25)\n",
        "tokenizer7 = Tokenizer(num_words=25)\n",
        "# This builds the word index\n",
        "tokenizer1.fit_on_texts(df['gender'])\n",
        "tokenizer2.fit_on_texts(df['masterCategory'])\n",
        "tokenizer3.fit_on_texts(df['subCategory'])\n",
        "tokenizer4.fit_on_texts(df['articleType'])\n",
        "tokenizer5.fit_on_texts(df['baseColour'])\n",
        "tokenizer6.fit_on_texts(df['season'])\n",
        "tokenizer7.fit_on_texts(df['usage'])\n",
        "\n",
        "\n",
        "# This turns strings into lists of integer indices.\n",
        "#train_sequences = tokenizer.texts_to_sequences(data.values())\n",
        "print(tokenizer1.word_index)\n",
        "print(tokenizer2.word_index)\n",
        "print(tokenizer3.word_index)\n",
        "print(tokenizer4.word_index)\n",
        "print(tokenizer5.word_index)\n",
        "print(tokenizer6.word_index)\n",
        "print(tokenizer7.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "240\n",
            "{'men': 1, 'women': 2, 'unisex': 3, 'boys': 4, 'girls': 5}\n",
            "{'apparel': 1, 'accessories': 2, 'footwear': 3, 'personal': 4, 'care': 5, 'free': 6, 'items': 7, 'sporting': 8, 'goods': 9, 'home': 10}\n",
            "{'topwear': 1, 'shoes': 2, 'bags': 3, 'bottomwear': 4, 'watches': 5, 'innerwear': 6, 'jewellery': 7, 'eyewear': 8, 'fragrance': 9, 'sandal': 10, 'wallets': 11, 'flip': 12, 'flops': 13, 'belts': 14, 'socks': 15, 'lips': 16, 'and': 17, 'dress': 18, 'loungewear': 19, 'nightwear': 20, 'saree': 21, 'nails': 22, 'makeup': 23, 'headwear': 24, 'ties': 25, 'accessories': 26, 'skin': 27, 'scarves': 28, 'cufflinks': 29, 'apparel': 30, 'set': 31, 'free': 32, 'gifts': 33, 'stoles': 34, 'care': 35, 'eyes': 36, 'mufflers': 37, 'shoe': 38, 'sports': 39, 'equipment': 40, 'gloves': 41, 'hair': 42, 'bath': 43, 'body': 44, 'water': 45, 'bottle': 46, 'perfumes': 47, 'umbrellas': 48, 'beauty': 49, 'wristbands': 50, 'home': 51, 'furnishing': 52, 'vouchers': 53}\n",
            "{'tshirts': 1, 'shoes': 2, 'shirts': 3, 'casual': 4, 'watches': 5, 'sports': 6, 'kurtas': 7, 'tops': 8, 'handbags': 9, 'heels': 10, 'sunglasses': 11, 'and': 12, 'sandals': 13, 'wallets': 14, 'flip': 15, 'flops': 16, 'briefs': 17, 'belts': 18, 'backpacks': 19, 'socks': 20, 'formal': 21, 'body': 22, 'perfume': 23, 'mist': 24, 'jeans': 25, 'shorts': 26, 'trousers': 27, 'flats': 28, 'bra': 29, 'dresses': 30, 'sarees': 31, 'earrings': 32, 'pants': 33, 'deodorant': 34, 'nail': 35, 'polish': 36, 'lipstick': 37, 'track': 38, 'clutches': 39, 'sweatshirts': 40, 'caps': 41, 'sweaters': 42, 'ties': 43, 'jackets': 44, 'innerwear': 45, 'vests': 46, 'set': 47, 'kurtis': 48, 'tunics': 49, 'bag': 50, 'lip': 51, 'nightdress': 52, 'leggings': 53, 'pendant': 54, 'capris': 55, 'gift': 56, 'necklace': 57, 'chains': 58, 'gloss': 59, 'suits': 60, 'night': 61, 'trunk': 62, 'accessory': 63, 'skirts': 64, 'dupatta': 65, 'scarves': 66, 'ring': 67, 'cufflinks': 68, 'kajal': 69, 'eyeliner': 70, 'lounge': 71, 'face': 72, 'kurta': 73, 'sets': 74, 'free': 75, 'gifts': 76, 'stoles': 77, 'duffel': 78, 'bangle': 79, 'laptop': 80, 'foundation': 81, 'primer': 82, 'bracelet': 83, 'pouch': 84, 'moisturisers': 85, 'jewellery': 86, 'fragrance': 87, 'highlighter': 88, 'blush': 89, 'boxers': 90, 'compact': 91, 'liner': 92, 'mobile': 93, 'messenger': 94, 'eyeshadow': 95, 'suspenders': 96, 'camisoles': 97, 'salwar': 98, 'mufflers': 99, 'patiala': 100, 'jeggings': 101, 'stockings': 102, 'churidar': 103, 'wash': 104, 'tracksuits': 105, 'cleanser': 106, 'sunscreen': 107, 'shoe': 108, 'robe': 109, 'accessories': 110, 'rain': 111, 'bath': 112, 'gloves': 113, 'hair': 114, 'colour': 115, 'jacket': 116, 'swimwear': 117, 'waist': 118, 'baby': 119, 'dolls': 120, 'travel': 121, 'care': 122, 'jumpsuit': 123, 'waistcoat': 124, 'basketballs': 125, 'mascara': 126, 'mask': 127, 'peel': 128, 'rompers': 129, 'booties': 130, 'water': 131, 'bottle': 132, 'concealer': 133, 'rucksacks': 134, 'tights': 135, 'shapewear': 136, 'blazers': 137, 'footballs': 138, 'clothing': 139, 'headband': 140, 'wristbands': 141, 'shrug': 142, 'eye': 143, 'cream': 144, 'essentials': 145, 'scrub': 146, 'lotion': 147, 'umbrellas': 148, 'exfoliator': 149, 'nehru': 150, 'toner': 151, 'beauty': 152, 'lehenga': 153, 'choli': 154, 'makeup': 155, 'remover': 156, 'plumper': 157, 'trolley': 158, 'tablet': 159, 'sleeve': 160, 'hat': 161, 'key': 162, 'chain': 163, 'serum': 164, 'gel': 165, 'laces': 166, 'cushion': 167, 'covers': 168, 'mens': 169, 'grooming': 170, 'kit': 171, 'ipad': 172}\n",
            "{'black': 1, 'blue': 2, 'white': 3, 'brown': 4, 'grey': 5, 'red': 6, 'green': 7, 'pink': 8, 'navy': 9, 'purple': 10, 'silver': 11, 'yellow': 12, 'beige': 13, 'gold': 14, 'maroon': 15, 'orange': 16, 'olive': 17, 'multi': 18, 'cream': 19, 'steel': 20, 'charcoal': 21, 'peach': 22, 'off': 23, 'skin': 24, 'lavender': 25, 'melange': 26, 'khaki': 27, 'magenta': 28, 'teal': 29, 'tan': 30, 'mustard': 31, 'bronze': 32, 'copper': 33, 'turquoise': 34, 'rust': 35, 'burgundy': 36, 'metallic': 37, 'coffee': 38, 'mauve': 39, 'rose': 40, 'nude': 41, 'sea': 42, 'mushroom': 43, 'dk': 44, 'taupe': 45, 'lime': 46, 'fluorescent': 47}\n",
            "{'summer': 1, 'fall': 2, 'winter': 3, 'spring': 4, 'dk': 5}\n",
            "{'casual': 1, 'sports': 2, 'ethnic': 3, 'formal': 4, 'dk': 5, 'smart': 6, 'party': 7, 'travel': 8, 'home': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HShpXgznnonk"
      },
      "source": [
        "da = {}\n",
        "ds = {}\n",
        "for k,v in data.items():\n",
        "  da[str(k)] =[]\n",
        "  da[str(k)].append(tokenizer1.texts_to_sequences([v.split(':')[0]])[0][0])\n",
        "  da[str(k)].append(tokenizer2.texts_to_sequences([v.split(':')[1]])[0][0])\n",
        "  da[str(k)].append(tokenizer3.texts_to_sequences([v.split(':')[2]])[0][0])\n",
        "  da[str(k)].append(tokenizer4.texts_to_sequences([v.split(':')[3]])[0][0])\n",
        "  da[str(k)].append(tokenizer5.texts_to_sequences([v.split(':')[4]])[0][0])\n",
        "  da[str(k)].append(tokenizer6.texts_to_sequences([v.split(':')[5]])[0][0])\n",
        "  da[str(k)].append(tokenizer7.texts_to_sequences([v.split(':')[6]])[0][0])\n",
        "  i=i+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVWO5yQVn0xE"
      },
      "source": [
        "def read_image(fn):\n",
        "  im = cv2.imread(fn)\n",
        "  im = cv2.cvtColor(im, cv2.COLOR_BGR2LAB)\n",
        "  im = cv2.resize(im, (128,128), interpolation=cv2.INTER_CUBIC)\n",
        "  im = np.array(im)/255\n",
        "  return im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akDOMlZGoF6s",
        "outputId": "161fe327-0b59-4b22-8ecd-d7fd3eab894d"
      },
      "source": [
        "traini = glob.glob(r'/content/drive/MyDrive/images/*')\n",
        "print(len(traini))\n",
        "def minibatch(data, batchsize):\n",
        "    length = len(data)\n",
        "    epoch = i = 0\n",
        "    tmpsize = None    \n",
        "    while True:\n",
        "        size = tmpsize if tmpsize else batchsize\n",
        "        if i+size > length:\n",
        "            shuffle(data)\n",
        "            i = 0\n",
        "            epoch+=1        \n",
        "        rtn = [read_image(data[j]) for j in range(i,i+size)]\n",
        "        ltn=[]\n",
        "        se = []\n",
        "        for j in range(i,i+size):\n",
        "          if '(' in data[j].split('/')[-1].split('.')[0]:\n",
        "            t=[]\n",
        "            p = data[j].split('/')[-1].split('.')[0].split(' ')[0]\n",
        "            t.append(da[p][0]/5)\n",
        "            t.append(da[p][1]/10)\n",
        "            t.append(da[p][2]/53)\n",
        "            t.append(da[p][3]/172)\n",
        "            t.append(da[p][4]/47)\n",
        "            t.append(da[p][5]/5)\n",
        "            t.append(da[p][6]/9)\n",
        "            se.append(t) \n",
        "          else:\n",
        "            t=[]\n",
        "            p = data[j].split('/')[-1].split('.')[0]\n",
        "            t.append(da[p][0]/5)\n",
        "            t.append(da[p][1]/10)\n",
        "            t.append(da[p][2]/53)\n",
        "            t.append(da[p][3]/172)\n",
        "            t.append(da[p][4]/47)\n",
        "            t.append(da[p][5]/5)\n",
        "            t.append(da[p][6]/9)\n",
        "            se.append(t) \n",
        "        #ltn = [data_enc_padded[data[j].split('/')[-1].split('.')[0]] for j in range(i,i+size)]\n",
        "        i+=size\n",
        "        tmpsize = yield epoch, np.float32(rtn), np.array(se)\n",
        "\n",
        "def minibatchAB(dataA, batchsize):\n",
        "    batchA = minibatch(dataA, batchsize)\n",
        "    tmpsize = None    \n",
        "    while True:        \n",
        "        ep1, A ,B = batchA.send(tmpsize)\n",
        "        tmpsize = yield ep1, A , B\n",
        "train_batch = minibatchAB(traini, 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM-zCiNgoMtC"
      },
      "source": [
        "class ClipConstraint(tf.keras.constraints.Constraint):\n",
        "  def __init__(self, clip_value):\n",
        "    self.clip_value = clip_value\n",
        "  def __call__(self, weights):\n",
        "    return backend.clip(weights, -self.clip_value, self.clip_value)\n",
        "  def get_config(self):\n",
        "    return {'clip_value': self.clip_value}\n",
        "const = ClipConstraint(0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP84E5yeoRGA"
      },
      "source": [
        "class CycleGAN():\n",
        "  def __init__(self,train_batch,const):\n",
        "    self.img_rows = 128 \n",
        "    self.img_cols = 128 \n",
        "    self.channels = 3 \n",
        "    self.dloss = []\n",
        "    self.gloss = []\n",
        "    self.iter = []\n",
        "    self.traindata = train_batch\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "    self.label_shape = (7,)\n",
        "    self.lambda_cycle = 10\n",
        "    self.bloss = tf.keras.losses.binary_crossentropy\n",
        "    optimizer = Adam(learning_rate = 0.002)\n",
        "    self.const = const\n",
        "    self.g_AB = self.build_label_generator() \n",
        "    self.g_BA = self.build_img_generator()\n",
        "    A = Input(shape=self.img_shape) \n",
        "    B = Input(shape=self.label_shape) \n",
        "    f_B = self.g_AB(A) \n",
        "    fake_B = concatenate(f_B,axis=1)\n",
        "    fake_A = self.g_BA(B)\n",
        "    reconstr_A = self.g_BA(fake_B) \n",
        "    r_B = self.g_AB(fake_A)\n",
        "    reconstr_B = concatenate(r_B,axis=1)\n",
        "    self.d_A = self.build_img_label_discriminator() \n",
        "    self.d_B = self.build_img_label_discriminator() \n",
        "    self.d_A.compile(loss=self.wa_loss,optimizer=RMSprop(0.00005)) \n",
        "    self.d_B.compile(loss=self.wa_loss,optimizer=RMSprop(0.00005))  \n",
        "    self.d_A.trainable = False \n",
        "    self.d_B.trainable = False\n",
        "    valid_A = self.d_A([fake_A,B]) \n",
        "    valid_B = self.d_B([A,fake_B])\n",
        "    self.classifier = Model(inputs = A,outputs = f_B)\n",
        "    self.classifier.compile(loss = [self.bloss,self.bloss,self.bloss,self.bloss,self.bloss,self.bloss,self.bloss],optimizer = RMSprop(0.00005))\n",
        "    self.combined = Model(inputs=[A,B],outputs=[valid_A,valid_B,reconstr_A]+r_B) \n",
        "    self.combined.compile(loss=[self.wa_loss,self.wa_loss,tf.keras.losses.binary_crossentropy,self.bloss,self.bloss,self.bloss,self.bloss,self.bloss,self.bloss,self.bloss],loss_weights=[1, 1,self.lambda_cycle, self.lambda_cycle],optimizer=RMSprop(0.00005))\n",
        "  def wa_loss(self,y_true, y_pred):\n",
        "    return K.mean(y_true * y_pred)\n",
        "  def build_img_generator(self):\n",
        "    d0 = Input(shape=self.label_shape)\n",
        "    d1 = Dense(128,activation = 'relu')(d0)\n",
        "    d2 = tf.keras.layers.Reshape((1,1,128))(d1)\n",
        "    d3 = Conv2DTranspose(128,kernel_size = 3,strides = 2,padding = 'same')(d2)\n",
        "    #d3 = BatchNormalization()(d3)\n",
        "    d3 = tfa.layers.InstanceNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(128,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = tfa.layers.InstanceNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(128,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = tfa.layers.InstanceNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(64,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = tfa.layers.InstanceNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(64,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = tfa.layers.InstanceNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(64,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = tfa.layers.InstanceNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(64,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = tfa.layers.InstanceNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(32,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(32,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(32,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = tfa.layers.InstanceNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(16,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = tfa.layers.InstanceNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(16,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = tfa.layers.InstanceNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(8,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    output_img = Conv2D(3,kernel_size = 3,strides = 1,padding = 'same',activation='sigmoid')(d3)\n",
        "    return Model(d0, output_img) \n",
        "  def build_label_generator(self):\n",
        "    d0 = Input(shape=self.img_shape)\n",
        "    d1 = Conv2D(32,kernel_size = 3,strides = 1,padding = 'same')(d0)\n",
        "    #d1 = BatchNormalization()(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(32,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(64,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(64,kernel_size = 3,strides = 1,padding = 'same')(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(64,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 1,padding = 'same')(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(256,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(512,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    df2 = Flatten()(d1)\n",
        "    d2 = Dense(512,activation = 'relu')(df2)\n",
        "    d2 = Dense(128,activation='relu')(d2)\n",
        "    d2 = Dense(32,activation='relu')(d2)\n",
        "    d2 = Dense(1,activation='sigmoid')(d2)\n",
        "    d3 = Dense(512,activation = 'relu')(df2)\n",
        "    d3 = Dense(128,activation='relu')(d3)\n",
        "    d3 = Dense(32,activation='relu')(d3)\n",
        "    d3 = Dense(1,activation='sigmoid')(d3)\n",
        "    d4 = Dense(512,activation = 'relu')(df2)\n",
        "    d4 = Dense(128,activation='relu')(d4)\n",
        "    d4 = Dense(32,activation='relu')(d4)\n",
        "    d4 = Dense(1,activation='sigmoid')(d4)\n",
        "    d5 = Dense(512,activation = 'relu')(df2)\n",
        "    d5 = Dense(128,activation='relu')(d5)\n",
        "    d5 = Dense(32,activation='relu')(d5)\n",
        "    d5 = Dense(1,activation='sigmoid')(d5)\n",
        "    d6 = Dense(512,activation = 'relu')(df2)\n",
        "    d6 = Dense(128,activation='relu')(d6)\n",
        "    d6 = Dense(32,activation='relu')(d6)\n",
        "    d6 = Dense(1,activation='sigmoid')(d6)\n",
        "    d7 = Dense(512,activation = 'relu')(df2)\n",
        "    d7 = Dense(128,activation='relu')(d7)\n",
        "    d7 = Dense(32,activation='relu')(d7)\n",
        "    d7 = Dense(1,activation='sigmoid')(d7)\n",
        "    d8 = Dense(512,activation = 'relu')(df2)\n",
        "    d8 = Dense(128,activation='relu')(d8)\n",
        "    d8 = Dense(32,activation='relu')(d8)\n",
        "    d8 = Dense(1,activation='sigmoid')(d8)\n",
        "    #output_label = Dense(12,activation='sigmoid')(d2)\n",
        "    return Model(d0, [d2,d3,d4,d5,d6,d7,d8])\n",
        "  def build_img_label_discriminator(self):\n",
        "    x1 = Input(shape=self.img_shape)\n",
        "    x2 = Input(shape=self.label_shape)\n",
        "    d1 = Conv2D(32,kernel_size = 3,strides = 4,kernel_constraint=self.const, padding = 'same')(x1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(64,kernel_size = 3,strides = 4,kernel_constraint=self.const,padding = 'same')(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 2,kernel_constraint=self.const,padding = 'same')(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 2,kernel_constraint=self.const,padding = 'same')(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(512,kernel_size = 3,strides = 2,kernel_constraint=self.const,padding = 'same')(d1)\n",
        "    d1 = tfa.layers.InstanceNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d2 = Flatten()(d1)\n",
        "    d4 = Dense(512)(x2)\n",
        "    d3 = concatenate([d4,d2])\n",
        "    d3 = LeakyReLU(0.2)(d3)\n",
        "    d3 = Dense(64)(d3)\n",
        "    d3 = LeakyReLU(0.2)(d3)\n",
        "    d3 = Dense(32)(d3)\n",
        "    d3 = LeakyReLU(0.2)(d3)\n",
        "    validity = Dense(1)(d3)\n",
        "    return Model(inputs = [x1,x2], outputs = [validity])\n",
        "  def train(self, epochs, batch_size=64, sample_interval=50):\n",
        "    valid = -np.ones((batch_size,1))\n",
        "    fake = np.ones((batch_size,1))\n",
        "    for epoch in range(epochs):\n",
        "      for i in range(5):\n",
        "        ep, A,B= next(self.traindata)\n",
        "        fake_A = self.g_BA.predict(B)\n",
        "        dA_loss_real = self.d_A.train_on_batch([A,B], valid) \n",
        "        dA_loss_fake = self.d_A.train_on_batch([fake_A,B], fake)\n",
        "        dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "        f_B = self.g_AB.predict(A)\n",
        "        fake_B = np.concatenate(f_B,axis=1)\n",
        "        dB_loss_real = self.d_B.train_on_batch([A,B], valid)\n",
        "        dB_loss_fake = self.d_B.train_on_batch([A,fake_B], fake)\n",
        "        dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "        o1,o2,o3,o4,o5,o6,o7 = [], [], [], [], [], [], []\n",
        "        for i in B[:]:\n",
        "          o1.append(i[0])\n",
        "          o2.append(i[1])\n",
        "          o3.append(i[2])\n",
        "          o4.append(i[3])\n",
        "          o5.append(i[4])\n",
        "          o6.append(i[5])\n",
        "          o7.append(i[6])\n",
        "        o1 = np.array(o1)\n",
        "        o2 = np.array(o2)\n",
        "        o3 = np.array(o3)\n",
        "        o4 = np.array(o4)\n",
        "        o5 = np.array(o5)\n",
        "        o6 = np.array(o6)\n",
        "        o7 = np.array(o7)\n",
        "        classifier_loss = self.classifier.train_on_batch(A,[o1,o2,o3,o4,o5,o6,o7])\n",
        "      d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
        "      ep, A,B= next(self.traindata)\n",
        "      o1,o2,o3,o4,o5,o6,o7 = [], [], [], [], [], [], []\n",
        "      for i in B[:]:\n",
        "        o1.append(i[0])\n",
        "        o2.append(i[1])\n",
        "        o3.append(i[2])\n",
        "        o4.append(i[3])\n",
        "        o5.append(i[4])\n",
        "        o6.append(i[5])\n",
        "        o7.append(i[6])\n",
        "      o1 = np.array(o1)\n",
        "      o2 = np.array(o2)\n",
        "      o3 = np.array(o3)\n",
        "      o4 = np.array(o4)\n",
        "      o5 = np.array(o5)\n",
        "      o6 = np.array(o6)\n",
        "      o7 = np.array(o7)\n",
        "      g_loss = self.combined.train_on_batch([A,B],[valid, valid, A,o1,o2,o3,o4,o5,o6,o7])\n",
        "      if epoch % sample_interval == 0: \n",
        "        self.dloss.append(np.mean(d_loss))\n",
        "        self.gloss.append(np.mean(g_loss))\n",
        "        self.iter.append(epoch)\n",
        "        print(\"%d [D loss : %f ] [G loss : %f]\"%(epoch,np.mean(d_loss),np.mean(g_loss)))\n",
        "        res = gan.g_AB.predict(np.array([A[0]]))\n",
        "        lk = [5,10,53,172,47,5,9]\n",
        "        r,b = [],[]\n",
        "        for i,v in enumerate(lk):\n",
        "          p = res[i]*v\n",
        "          r.append(p[0][0])\n",
        "          b.append(B[0][i]*v)\n",
        "        print(b)\n",
        "        print(np.round(np.array(r)))\n",
        "      if epoch % 10 == 0:\n",
        "        self.g_AB.save('/content/drive/MyDrive/g_ABclass')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJrjPek2oYOh"
      },
      "source": [
        "gan = CycleGAN(train_batch,const)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdeIcJqtoYyD",
        "outputId": "4fb047a8-f7dc-4fe0-cee4-ef0a2658d693"
      },
      "source": [
        "gan.train(epochs=1500, batch_size=64, sample_interval=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [D loss : -0.000388 ] [G loss : 1.583081]\n",
            "[1.0, 3.0, 2.0, 6.0, 2.9999999999999996, 2.0, 2.0]\n",
            "[ 1.  3.  9. 29.  8.  2.  3.]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/g_ABclass/assets\n",
            "1 [D loss : 0.000843 ] [G loss : 1.552821]\n",
            "[1.0, 1.0, 4.0, 27.0, 5.0, 2.0, 4.0]\n",
            "[ 2.  1.  4. 11.  6.  2.  2.]\n",
            "2 [D loss : -0.000347 ] [G loss : 1.663343]\n",
            "[1.0, 1.0, 1.0, 44.0, 2.0, 1.0, 1.0]\n",
            "[1. 1. 3. 7. 6. 2. 1.]\n",
            "3 [D loss : -0.000498 ] [G loss : 1.525469]\n",
            "[1.0, 1.0, 1.0, 1.0, 7.0, 2.0, 1.0]\n",
            "[1. 1. 2. 8. 6. 2. 1.]\n",
            "4 [D loss : 0.000134 ] [G loss : 1.539349]\n",
            "[1.0, 1.0, 1.0, 3.0, 2.9999999999999996, 2.0, 1.0]\n",
            "[1. 1. 1. 6. 5. 2. 2.]\n",
            "5 [D loss : -0.001195 ] [G loss : 1.508403]\n",
            "[1.0, 1.0, 4.0, 26.0, 5.0, 2.0, 2.0]\n",
            "[ 2.  1.  3. 13.  7.  2.  2.]\n",
            "6 [D loss : -0.001502 ] [G loss : 1.516699]\n",
            "[1.0, 1.0, 1.0, 1.0, 29.0, 2.0, 1.0]\n",
            "[1. 1. 2. 7. 6. 1. 1.]\n",
            "7 [D loss : -0.001714 ] [G loss : 1.557344]\n",
            "[1.0, 2.0, 29.0, 68.0, 20.0, 1.0, 1.0]\n",
            "[ 2.  3.  6. 12.  5.  2.  1.]\n",
            "8 [D loss : -0.002589 ] [G loss : 1.530446]\n",
            "[2.0, 2.0, 7.0, 83.0, 11.0, 1.0, 1.0]\n",
            "[ 2.  3. 10. 22.  9.  2.  2.]\n",
            "9 [D loss : -0.003424 ] [G loss : 1.534502]\n",
            "[1.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0]\n",
            "[ 1.  1.  1.  5. 10.  1.  1.]\n",
            "10 [D loss : -0.003129 ] [G loss : 1.553769]\n",
            "[2.0, 6.0, 32.0, 75.0, 30.999999999999996, 1.0, 1.0]\n",
            "[ 2.  3.  9. 24.  9.  2.  1.]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/g_ABclass/assets\n",
            "11 [D loss : -0.004190 ] [G loss : 1.521329]\n",
            "[1.0, 2.0, 15.0, 20.0, 9.0, 1.0, 1.0]\n",
            "[ 2.  1.  7. 25.  6.  2.  2.]\n",
            "12 [D loss : -0.004371 ] [G loss : 1.538354]\n",
            "[2.0, 3.0, 2.0, 4.0, 8.0, 3.0, 1.0]\n",
            "[ 2.  3.  4. 10.  8.  2.  1.]\n",
            "13 [D loss : -0.002503 ] [G loss : 1.554027]\n",
            "[1.0, 1.0, 4.0, 25.0, 2.0, 1.0, 1.0]\n",
            "[ 2.  1.  5. 20.  7.  2.  1.]\n",
            "14 [D loss : -0.002913 ] [G loss : 1.580200]\n",
            "[2.0, 1.0, 1.0, 1.0, 5.999999999999999, 1.0, 1.0]\n",
            "[ 2.  2.  4. 13. 11.  1.  2.]\n",
            "15 [D loss : -0.004943 ] [G loss : 1.571610]\n",
            "[2.0, 1.0, 1.0, 49.0, 4.0, 1.0, 1.0]\n",
            "[1. 1. 2. 8. 7. 1. 2.]\n",
            "16 [D loss : -0.002136 ] [G loss : 1.558120]\n",
            "[2.0, 4.0, 16.0, 37.0, 4.0, 4.0, 1.0]\n",
            "[ 2.  5. 15. 40. 11.  4.  1.]\n",
            "17 [D loss : -0.003402 ] [G loss : 1.559010]\n",
            "[1.0, 3.0, 2.0, 4.0, 4.0, 3.0, 1.0]\n",
            "[ 1.  3.  4. 13.  5.  3.  1.]\n",
            "18 [D loss : -0.004862 ] [G loss : 1.563014]\n",
            "[4.0, 1.0, 1.0, 1.0, 7.0, 1.0, 1.0]\n",
            "[ 2.  3. 18. 41. 11.  3.  1.]\n",
            "19 [D loss : -0.003600 ] [G loss : 1.535673]\n",
            "[1.0, 1.0, 1.0, 1.0, 5.0, 1.0, 2.0]\n",
            "[ 2.  1.  4. 26.  6.  1.  2.]\n",
            "20 [D loss : -0.004010 ] [G loss : 1.552178]\n",
            "[2.0, 3.0, 12.0, 15.0, 2.0, 1.0, 1.0]\n",
            "[ 2.  3.  4. 13. 10.  3.  1.]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/g_ABclass/assets\n",
            "21 [D loss : -0.004170 ] [G loss : 1.540358]\n",
            "[2.0, 1.0, 21.0, 30.999999999999996, 10.0, 2.0, 3.0]\n",
            "[ 2.  1. 11. 28. 10.  2.  2.]\n",
            "22 [D loss : -0.006438 ] [G loss : 1.519470]\n",
            "[1.0, 1.0, 6.0, 17.0, 2.9999999999999996, 1.0, 1.0]\n",
            "[ 2.  2.  7. 19.  9.  2.  1.]\n",
            "23 [D loss : -0.007335 ] [G loss : 1.502217]\n",
            "[2.0, 1.0, 1.0, 1.0, 46.0, 1.0, 1.0]\n",
            "[ 2.  1. 11. 32.  9.  1.  1.]\n",
            "24 [D loss : -0.007033 ] [G loss : 1.543998]\n",
            "[1.0, 3.0, 2.0, 21.0, 1.0, 1.0, 4.0]\n",
            "[1. 2. 2. 6. 3. 1. 2.]\n",
            "25 [D loss : -0.005866 ] [G loss : 1.517452]\n",
            "[2.0, 2.0, 11.0, 14.000000000000002, 38.0, 2.0, 1.0]\n",
            "[ 2.  2.  4. 20.  5.  1.  1.]\n",
            "26 [D loss : -0.008701 ] [G loss : 1.511083]\n",
            "[2.0, 1.0, 1.0, 1.0, 8.0, 1.0, 1.0]\n",
            "[ 2.  1.  4. 26. 15.  1.  2.]\n",
            "27 [D loss : -0.008586 ] [G loss : 1.512338]\n",
            "[2.0, 2.0, 7.0, 56.99999999999999, 18.0, 3.0, 1.0]\n",
            "[ 2.  1.  7. 40.  8.  2.  1.]\n",
            "28 [D loss : -0.007366 ] [G loss : 1.480437]\n",
            "[1.0, 1.0, 6.0, 61.99999999999999, 9.0, 1.0, 1.0]\n",
            "[ 2.  1.  8. 26.  6.  1.  1.]\n",
            "29 [D loss : -0.006375 ] [G loss : 1.485366]\n",
            "[1.0, 3.0, 2.0, 21.0, 4.0, 1.0, 4.0]\n",
            "[1. 2. 3. 9. 4. 3. 2.]\n",
            "30 [D loss : -0.005401 ] [G loss : 1.468714]\n",
            "[2.0, 1.0, 1.0, 7.000000000000001, 29.0, 1.0, 3.0]\n",
            "[ 2.  1.  4. 31.  7.  1.  2.]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/g_ABclass/assets\n",
            "31 [D loss : -0.007603 ] [G loss : 1.493958]\n",
            "[2.0, 2.0, 7.0, 32.0, 11.0, 1.0, 1.0]\n",
            "[ 2.  1.  6. 33. 12.  1.  1.]\n",
            "32 [D loss : -0.010250 ] [G loss : 1.503146]\n",
            "[1.0, 2.0, 5.0, 5.0, 1.0, 3.0, 1.0]\n",
            "[ 2.  2.  6. 25.  6.  2.  2.]\n",
            "33 [D loss : -0.006162 ] [G loss : 1.464109]\n",
            "[1.0, 2.0, 5.0, 5.0, 1.0, 3.0, 1.0]\n",
            "[ 2.  2.  4. 11.  5.  3.  1.]\n",
            "34 [D loss : -0.009117 ] [G loss : 1.458839]\n",
            "[1.0, 2.0, 5.0, 5.0, 2.9999999999999996, 3.0, 1.0]\n",
            "[ 2.  2.  5. 23.  9.  2.  1.]\n",
            "35 [D loss : -0.008444 ] [G loss : 1.495587]\n",
            "[1.0, 2.0, 24.0, 41.0, 2.0, 2.0, 1.0]\n",
            "[ 1.  3.  8. 31.  9.  2.  1.]\n",
            "36 [D loss : -0.008786 ] [G loss : 1.462608]\n",
            "[2.0, 1.0, 1.0, 48.0, 2.9999999999999996, 1.0, 3.0]\n",
            "[1. 1. 1. 7. 5. 1. 1.]\n",
            "37 [D loss : -0.008262 ] [G loss : 1.446384]\n",
            "[5.0, 3.0, 12.0, 15.0, 10.0, 3.0, 1.0]\n",
            "[ 2.  3.  6. 24.  8.  2.  1.]\n",
            "38 [D loss : -0.009751 ] [G loss : 1.433897]\n",
            "[1.0, 1.0, 4.0, 27.0, 1.0, 1.0, 1.0]\n",
            "[ 1.  2.  3. 25.  4.  2.  2.]\n",
            "39 [D loss : -0.008597 ] [G loss : 1.506320]\n",
            "[2.0, 3.0, 2.0, 6.0, 1.0, 1.0, 2.0]\n",
            "[ 1.  3.  6. 11.  5.  2.  1.]\n",
            "40 [D loss : -0.007254 ] [G loss : 1.469349]\n",
            "[2.0, 2.0, 7.0, 32.0, 14.0, 3.0, 1.0]\n",
            "[ 2.  2.  5. 24.  8.  2.  1.]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/g_ABclass/assets\n",
            "41 [D loss : -0.007370 ] [G loss : 1.434260]\n",
            "[1.0, 1.0, 1.0, 1.0, 5.0, 1.0, 1.0]\n",
            "[1. 1. 1. 6. 5. 1. 1.]\n",
            "42 [D loss : -0.009217 ] [G loss : 1.440300]\n",
            "[2.0, 2.0, 3.0, 9.0, 5.999999999999999, 1.0, 1.0]\n",
            "[ 2.  3.  6. 18.  7.  1.  1.]\n",
            "43 [D loss : -0.008285 ] [G loss : 1.424727]\n",
            "[3.0, 3.0, 2.0, 4.0, 5.999999999999999, 1.0, 1.0]\n",
            "[1. 3. 4. 7. 6. 1. 1.]\n",
            "44 [D loss : -0.009665 ] [G loss : 1.462484]\n",
            "[2.0, 1.0, 1.0, 49.0, 22.0, 1.0, 1.0]\n",
            "[ 2.  1.  3. 19. 11.  1.  2.]\n",
            "45 [D loss : -0.008571 ] [G loss : 1.419482]\n",
            "[5.0, 1.0, 18.0, 30.0, 8.0, 2.0, 1.0]\n",
            "[ 2.  2.  8. 23.  7.  2.  1.]\n",
            "46 [D loss : -0.008572 ] [G loss : 1.412022]\n",
            "[1.0, 4.0, 9.0, 22.999999999999996, 16.0, 4.0, 1.0]\n",
            "[ 2.  2.  8. 41.  7.  3.  1.]\n",
            "47 [D loss : -0.009024 ] [G loss : 1.387270]\n",
            "[2.0, 1.0, 1.0, 65.0, 10.0, 2.0, 3.0]\n",
            "[ 2.  1.  5. 20.  5.  1.  2.]\n",
            "48 [D loss : -0.011136 ] [G loss : 1.413319]\n",
            "[1.0, 1.0, 1.0, 1.0, 15.0, 1.0, 1.0]\n",
            "[ 1.  1.  0.  3. 10.  1.  1.]\n",
            "49 [D loss : -0.010199 ] [G loss : 1.361946]\n",
            "[2.0, 2.0, 14.0, 18.0, 10.0, 1.0, 1.0]\n",
            "[ 1.  2. 10. 30.  3.  1.  2.]\n",
            "50 [D loss : -0.011156 ] [G loss : 1.390005]\n",
            "[2.0, 1.0, 1.0, 7.000000000000001, 15.0, 1.0, 3.0]\n",
            "[ 2.  1.  2. 10.  7.  1.  2.]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/g_ABclass/assets\n",
            "51 [D loss : -0.009898 ] [G loss : 1.402357]\n",
            "[1.0, 3.0, 2.0, 4.0, 1.0, 1.0, 1.0]\n",
            "[1. 3. 2. 5. 5. 2. 2.]\n",
            "52 [D loss : -0.010904 ] [G loss : 1.415809]\n",
            "[5.0, 1.0, 4.0, 26.0, 2.0, 1.0, 1.0]\n",
            "[ 3.  1.  5. 24.  3.  2.  1.]\n",
            "53 [D loss : -0.008959 ] [G loss : 1.380912]\n",
            "[1.0, 2.0, 5.0, 5.0, 2.9999999999999996, 3.0, 1.0]\n",
            "[3. 1. 4. 8. 6. 2. 1.]\n",
            "54 [D loss : -0.010230 ] [G loss : 1.420769]\n",
            "[1.0, 2.0, 5.0, 5.0, 2.9999999999999996, 3.0, 1.0]\n",
            "[1. 2. 4. 8. 7. 3. 1.]\n",
            "55 [D loss : -0.009981 ] [G loss : 1.373114]\n",
            "[1.0, 1.0, 1.0, 3.0, 2.0, 1.0, 1.0]\n",
            "[1. 1. 1. 3. 5. 1. 2.]\n",
            "56 [D loss : -0.011604 ] [G loss : 1.361188]\n",
            "[1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
            "[1. 1. 1. 3. 5. 1. 2.]\n",
            "57 [D loss : -0.012291 ] [G loss : 1.377676]\n",
            "[1.0, 2.0, 5.0, 5.0, 1.0, 3.0, 1.0]\n",
            "[ 2.  2.  5. 18.  5.  3.  1.]\n",
            "58 [D loss : -0.010408 ] [G loss : 1.390299]\n",
            "[2.0, 4.0, 9.0, 22.999999999999996, 2.9999999999999996, 4.0, 1.0]\n",
            "[ 2.  2.  6. 13.  8.  2.  1.]\n",
            "59 [D loss : -0.009383 ] [G loss : 1.349236]\n",
            "[1.0, 3.0, 2.0, 21.0, 1.0, 1.0, 4.0]\n",
            "[1. 3. 3. 9. 3. 2. 2.]\n",
            "60 [D loss : -0.012384 ] [G loss : 1.409832]\n",
            "[2.0, 1.0, 21.0, 30.999999999999996, 13.0, 1.0, 3.0]\n",
            "[ 2.  1. 14. 43. 14.  1.  2.]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/g_ABclass/assets\n",
            "61 [D loss : -0.010194 ] [G loss : 1.414269]\n",
            "[2.0, 1.0, 1.0, 65.0, 5.999999999999999, 1.0, 3.0]\n",
            "[ 2.  1.  8. 56. 13.  1.  3.]\n",
            "62 [D loss : -0.008346 ] [G loss : 1.417734]\n",
            "[1.0, 3.0, 2.0, 21.0, 1.0, 1.0, 4.0]\n",
            "[1. 3. 3. 9. 3. 1. 3.]\n",
            "63 [D loss : -0.010796 ] [G loss : 1.390955]\n",
            "[2.0, 2.0, 24.0, 41.0, 8.0, 1.0, 1.0]\n",
            "[ 1.  2.  6. 24.  5.  2.  1.]\n",
            "64 [D loss : -0.013328 ] [G loss : 1.351004]\n",
            "[1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
            "[1. 1. 1. 5. 6. 1. 1.]\n",
            "65 [D loss : -0.007433 ] [G loss : 1.377067]\n",
            "[2.0, 1.0, 1.0, 7.000000000000001, 13.0, 1.0, 3.0]\n",
            "[ 2.  1.  1. 16. 12.  1.  3.]\n",
            "66 [D loss : -0.011472 ] [G loss : 1.424931]\n",
            "[2.0, 3.0, 2.0, 10.0, 5.0, 3.0, 1.0]\n",
            "[ 1.  3.  5. 11.  6.  3.  1.]\n",
            "67 [D loss : -0.011979 ] [G loss : 1.376554]\n",
            "[2.0, 1.0, 6.0, 29.0, 23.999999999999996, 4.0, 1.0]\n",
            "[ 2.  1.  3. 33. 11.  1.  2.]\n",
            "68 [D loss : -0.007412 ] [G loss : 1.304348]\n",
            "[1.0, 1.0, 1.0, 1.0, 4.0, 2.0, 1.0]\n",
            "[1. 1. 1. 9. 6. 1. 1.]\n",
            "69 [D loss : -0.005761 ] [G loss : 1.413288]\n",
            "[1.0, 2.0, 5.0, 5.0, 1.0, 3.0, 1.0]\n",
            "[ 1.  1.  5. 10.  3.  2.  1.]\n",
            "70 [D loss : -0.006927 ] [G loss : 1.357645]\n",
            "[1.0, 3.0, 12.0, 15.0, 9.0, 1.0, 1.0]\n",
            "[ 2.  3.  5. 11.  4.  2.  1.]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/g_ABclass/assets\n",
            "71 [D loss : -0.005843 ] [G loss : 1.366651]\n",
            "[2.0, 2.0, 5.0, 5.0, 2.9999999999999996, 3.0, 1.0]\n",
            "[2. 2. 6. 8. 7. 3. 1.]\n",
            "72 [D loss : -0.016272 ] [G loss : 1.378491]\n",
            "[1.0, 1.0, 1.0, 1.0, 2.9999999999999996, 1.0, 1.0]\n",
            "[ 3.  2.  4. 28.  5.  1.  2.]\n",
            "73 [D loss : -0.001675 ] [G loss : 1.360679]\n",
            "[1.0, 2.0, 8.0, 11.0, 37.0, 3.0, 1.0]\n",
            "[ 1.  2.  6.  8. 13.  3.  1.]\n",
            "74 [D loss : -0.008765 ] [G loss : 1.409064]\n",
            "[1.0, 2.0, 3.0, 19.0, 1.0, 1.0, 1.0]\n",
            "[ 2.  1.  3. 21.  4.  1.  2.]\n",
            "75 [D loss : -0.011868 ] [G loss : 1.396860]\n",
            "[1.0, 1.0, 1.0, 3.0, 2.0, 1.0, 4.0]\n",
            "[ 2.  1.  5. 28.  5.  1.  1.]\n",
            "76 [D loss : -0.012153 ] [G loss : 1.377778]\n",
            "[1.0, 1.0, 1.0, 1.0, 7.0, 2.0, 1.0]\n",
            "[ 1.  1.  1.  3. 10.  1.  1.]\n",
            "77 [D loss : -0.010786 ] [G loss : 1.335733]\n",
            "[1.0, 2.0, 11.0, 14.000000000000002, 1.0, 3.0, 1.0]\n",
            "[ 1.  2.  7. 15.  2.  3.  1.]\n",
            "78 [D loss : -0.012683 ] [G loss : 1.388936]\n",
            "[1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]\n",
            "[1. 1. 1. 4. 4. 1. 2.]\n",
            "79 [D loss : -0.010924 ] [G loss : 1.338046]\n",
            "[1.0, 3.0, 2.0, 4.0, 1.0, 1.0, 1.0]\n",
            "[ 1.  3.  3. 10.  2.  2.  2.]\n",
            "80 [D loss : -0.005887 ] [G loss : 1.360203]\n",
            "[1.0, 3.0, 2.0, 4.0, 4.0, 2.0, 1.0]\n",
            "[1. 3. 2. 6. 2. 2. 2.]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/g_ABclass/assets\n",
            "81 [D loss : -0.017159 ] [G loss : 1.401736]\n",
            "[2.0, 2.0, 3.0, 9.0, 23.0, 1.0, 1.0]\n",
            "[ 2.  2.  4. 11. 10.  3.  1.]\n",
            "82 [D loss : -0.011075 ] [G loss : 1.380341]\n",
            "[1.0, 2.0, 14.0, 18.0, 4.0, 3.0, 1.0]\n",
            "[ 1.  2. 12. 13.  3.  2.  1.]\n",
            "83 [D loss : -0.012364 ] [G loss : 1.350868]\n",
            "[1.0, 3.0, 2.0, 21.0, 38.0, 2.0, 4.0]\n",
            "[ 1.  3.  3. 15.  5.  2.  2.]\n",
            "84 [D loss : -0.010793 ] [G loss : 1.346257]\n",
            "[1.0, 3.0, 2.0, 4.0, 1.0, 2.0, 1.0]\n",
            "[1. 3. 3. 5. 2. 2. 1.]\n",
            "85 [D loss : -0.014398 ] [G loss : 1.368169]\n",
            "[1.0, 1.0, 1.0, 40.0, 21.0, 2.0, 1.0]\n",
            "[1. 1. 1. 8. 5. 2. 2.]\n",
            "86 [D loss : -0.008711 ] [G loss : 1.364987]\n",
            "[1.0, 1.0, 4.0, 38.0, 5.0, 2.0, 1.0]\n",
            "[ 1.  1.  8. 35.  5.  2.  2.]\n",
            "87 [D loss : -0.016682 ] [G loss : 1.340557]\n",
            "[1.0, 1.0, 4.0, 25.0, 2.0, 1.0, 1.0]\n",
            "[ 1.  1.  3. 44.  4.  2.  2.]\n",
            "88 [D loss : -0.008083 ] [G loss : 1.386474]\n",
            "[2.0, 1.0, 4.0, 38.0, 1.0, 2.0, 2.0]\n",
            "[ 1.  1.  5. 27.  5.  2.  1.]\n",
            "89 [D loss : -0.008782 ] [G loss : 1.397859]\n",
            "[1.0, 2.0, 8.0, 11.0, 1.0, 3.0, 1.0]\n",
            "[ 1.  2.  6.  7. 10.  3.  1.]\n",
            "90 [D loss : -0.012983 ] [G loss : 1.380321]\n",
            "[2.0, 3.0, 2.0, 10.0, 2.0, 3.0, 1.0]\n",
            "[ 2.  3.  6. 15. 12.  2.  1.]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/g_ABclass/assets\n",
            "91 [D loss : -0.008504 ] [G loss : 1.383527]\n",
            "[3.0, 2.0, 3.0, 19.0, 1.0, 1.0, 1.0]\n",
            "[ 2.  2. 14. 48.  4.  2.  1.]\n",
            "92 [D loss : -0.006961 ] [G loss : 1.379697]\n",
            "[1.0, 1.0, 1.0, 3.0, 4.0, 3.0, 1.0]\n",
            "[1. 1. 1. 5. 8. 2. 1.]\n",
            "93 [D loss : -0.010636 ] [G loss : 1.357610]\n",
            "[1.0, 1.0, 1.0, 1.0, 2.9999999999999996, 2.0, 1.0]\n",
            "[1. 1. 1. 4. 4. 2. 1.]\n",
            "94 [D loss : -0.012133 ] [G loss : 1.333346]\n",
            "[3.0, 2.0, 24.0, 41.0, 1.0, 1.0, 1.0]\n",
            "[ 2.  2.  8. 29.  6.  2.  1.]\n",
            "95 [D loss : -0.011611 ] [G loss : 1.336058]\n",
            "[2.0, 2.0, 5.0, 5.0, 1.0, 3.0, 1.0]\n",
            "[ 1.  2.  8. 14.  6.  3.  1.]\n",
            "96 [D loss : -0.012414 ] [G loss : 1.330851]\n",
            "[1.0, 1.0, 6.0, 45.0, 5.0, 1.0, 1.0]\n",
            "[1. 1. 1. 6. 6. 2. 1.]\n",
            "97 [D loss : -0.012080 ] [G loss : 1.338424]\n",
            "[2.0, 1.0, 1.0, 42.0, 10.0, 2.0, 1.0]\n",
            "[ 2.  1.  1. 17. 10.  2.  2.]\n",
            "98 [D loss : -0.012886 ] [G loss : 1.330729]\n",
            "[3.0, 3.0, 2.0, 4.0, 2.0, 1.0, 1.0]\n",
            "[ 1.  3.  4. 10.  3.  2.  1.]\n",
            "99 [D loss : -0.010478 ] [G loss : 1.318884]\n",
            "[1.0, 1.0, 1.0, 1.0, 21.0, 1.0, 1.0]\n",
            "[1. 1. 1. 3. 7. 2. 1.]\n",
            "100 [D loss : -0.008224 ] [G loss : 1.307318]\n",
            "[1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0]\n",
            "[1. 1. 1. 5. 7. 2. 2.]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/g_ABclass/assets\n",
            "101 [D loss : -0.010401 ] [G loss : 1.300365]\n",
            "[1.0, 1.0, 4.0, 25.0, 2.0, 2.0, 1.0]\n",
            "[ 1.  1.  4. 35.  3.  2.  2.]\n",
            "102 [D loss : -0.013251 ] [G loss : 1.322996]\n",
            "[1.0, 1.0, 4.0, 27.0, 4.0, 2.0, 1.0]\n",
            "[ 1.  1.  5. 37.  5.  2.  2.]\n",
            "103 [D loss : -0.014431 ] [G loss : 1.295235]\n",
            "[1.0, 1.0, 4.0, 105.0, 5.0, 2.0, 2.0]\n",
            "[ 2.  1. 18. 77.  7.  2.  3.]\n",
            "104 [D loss : -0.011272 ] [G loss : 1.322142]\n",
            "[1.0, 1.0, 1.0, 40.0, 9.0, 2.0, 1.0]\n",
            "[1. 1. 1. 6. 3. 2. 2.]\n",
            "105 [D loss : -0.013689 ] [G loss : 1.320701]\n",
            "[1.0, 1.0, 1.0, 44.0, 2.0, 2.0, 1.0]\n",
            "[ 1.  1.  1. 14.  5.  2.  2.]\n",
            "106 [D loss : -0.008520 ] [G loss : 1.319771]\n",
            "[2.0, 1.0, 4.0, 38.0, 1.0, 2.0, 2.0]\n",
            "[ 1.  1.  7. 38.  3.  2.  2.]\n",
            "107 [D loss : -0.009347 ] [G loss : 1.318408]\n",
            "[1.0, 1.0, 1.0, 1.0, 5.0, 2.0, 1.0]\n",
            "[1. 1. 1. 8. 5. 2. 1.]\n",
            "108 [D loss : -0.010904 ] [G loss : 1.306823]\n",
            "[1.0, 1.0, 1.0, 3.0, 10.0, 2.0, 1.0]\n",
            "[ 1.  1.  2. 12.  3.  2.  1.]\n",
            "109 [D loss : -0.011038 ] [G loss : 1.316040]\n",
            "[1.0, 1.0, 1.0, 1.0, 11.999999999999998, 2.0, 1.0]\n",
            "[ 1.  1.  1.  7. 12.  1.  1.]\n",
            "110 [D loss : -0.001112 ] [G loss : 1.319360]\n",
            "[2.0, 1.0, 1.0, 7.000000000000001, 7.0, 2.0, 3.0]\n",
            "[2. 1. 1. 5. 7. 2. 4.]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/g_ABclass/assets\n",
            "111 [D loss : -0.009032 ] [G loss : 1.333085]\n",
            "[4.0, 3.0, 2.0, 4.0, 4.0, 2.0, 1.0]\n",
            "[1. 3. 4. 6. 2. 2. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}