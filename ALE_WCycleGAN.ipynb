{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALE_WCycleGAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOwTXLgGHYnJ0hLadDaspjt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RVijayChiranjithReddy/Autolabel_encoder/blob/main/ALE_WCycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ3DsF2gTT36",
        "outputId": "3fa7b5aa-ccbe-432d-b068-8f1c8f6715f7"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import *\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.activations import *\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.applications import *\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import glob\n",
        "from random import randint, shuffle\n",
        "%matplotlib inline\n",
        "K.set_learning_phase(1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:400: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiC1CpTdTb-4",
        "outputId": "d0f566c0-8beb-425b-c0d3-2d08771e8fe8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CUE6JV0Tew2",
        "outputId": "870828b6-bf22-455c-8d68-a0f38181a90c"
      },
      "source": [
        "import pandas\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "df = pandas.read_csv('/content/styles.csv')\n",
        "df.fillna('DK', inplace = True) \n",
        "data = {}\n",
        "for i in range(44446):\n",
        "  tags = [str(x) for x in df.loc[i].values[1:9] if type(x) is str]\n",
        "  #desc = df.loc[i].values[9]\n",
        "  #if type(desc) is not float:\n",
        "    #for de in desc.split():\n",
        "     #if de.isalpha() and de not in tags:\n",
        "        #tags.append(de)\n",
        "  data[str(df.loc[i].values[0])] = ':'.join(tags)\n",
        "vocab = []\n",
        "for k,v in data.items():\n",
        "  temp = list(set(v.split(':')))\n",
        "  vocab = vocab + temp\n",
        "  vocab = list(set(vocab))\n",
        "print(len(vocab))\n",
        "#from tensorflow.keras.preprocessing.text import one_hot\n",
        "#for k,d in data.items():\n",
        "  #data_encoded[k] = one_hot(' '.join(d),len(vocab))\n",
        "tokenizer = Tokenizer(num_words=len(vocab))\n",
        "\n",
        "# This builds the word index\n",
        "tokenizer.fit_on_texts(data.values())\n",
        "\n",
        "\n",
        "# This turns strings into lists of integer indices.\n",
        "train_sequences = tokenizer.texts_to_sequences(data.values())\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "240\n",
            "{'casual': 1, 'men': 2, 'apparel': 3, 'summer': 4, 'women': 5, 'topwear': 6, 'shoes': 7, 'accessories': 8, 'fall': 9, 'black': 10, 'footwear': 11, 'winter': 12, 'tshirts': 13, 'blue': 14, 'sports': 15, 'white': 16, 'watches': 17, 'brown': 18, 'shirts': 19, 'ethnic': 20, 'bags': 21, 'formal': 22, 'spring': 23, 'grey': 24, 'bottomwear': 25, 'care': 26, 'red': 27, 'personal': 28, 'unisex': 29, 'green': 30, 'innerwear': 31, 'wallets': 32, 'pink': 33, 'kurtas': 34, 'flip': 35, 'flops': 36, 'navy': 37, 'tops': 38, 'handbags': 39, 'purple': 40, 'belts': 41, 'and': 42, 'socks': 43, 'heels': 44, 'jewellery': 45, 'silver': 46, 'eyewear': 47, 'sunglasses': 48, 'fragrance': 49, 'sandals': 50, 'sandal': 51, 'briefs': 52, 'boys': 53, 'yellow': 54, 'beige': 55, 'backpacks': 56, 'girls': 57, 'body': 58, 'gold': 59, 'perfume': 60, 'mist': 61, 'jeans': 62, 'shorts': 63, 'maroon': 64, 'trousers': 65, 'orange': 66, 'lips': 67, 'ties': 68, 'flats': 69, 'dress': 70, 'bra': 71, 'loungewear': 72, 'nightwear': 73, 'dresses': 74, 'saree': 75, 'sarees': 76, 'earrings': 77, 'olive': 78, 'cream': 79, 'multi': 80, 'pants': 81, 'dk': 82, 'deodorant': 83, 'set': 84, 'nail': 85, 'nails': 86, 'polish': 87, 'skin': 88, 'lipstick': 89, 'steel': 90, 'makeup': 91, 'track': 92, 'free': 93, 'headwear': 94, 'clutches': 95, 'sweatshirts': 96, 'caps': 97, 'sweaters': 98, 'jackets': 99, 'vests': 100, 'scarves': 101, 'kurtis': 102, 'tunics': 103, 'charcoal': 104, 'bag': 105, 'cufflinks': 106, 'lip': 107, 'peach': 108, 'gifts': 109, 'nightdress': 110, 'off': 111, 'stoles': 112, 'leggings': 113, 'pendant': 114, 'capris': 115, 'gift': 116, 'lavender': 117, 'necklace': 118, 'chains': 119, 'melange': 120, 'gloss': 121, 'suits': 122, 'night': 123, 'trunk': 124, 'khaki': 125, 'accessory': 126, 'magenta': 127, 'skirts': 128, 'dupatta': 129, 'teal': 130, 'ring': 131, 'tan': 132, 'items': 133, 'kajal': 134, 'eyeliner': 135, 'lounge': 136, 'mustard': 137, 'face': 138, 'bronze': 139, 'kurta': 140, 'sets': 141, 'duffel': 142, 'copper': 143, 'bangle': 144, 'laptop': 145, 'mufflers': 146, 'foundation': 147, 'primer': 148, 'turquoise': 149, 'smart': 150, 'bracelet': 151, 'rust': 152, 'pouch': 153, 'moisturisers': 154, 'highlighter': 155, 'blush': 156, 'boxers': 157, 'compact': 158, 'shoe': 159, 'liner': 160, 'mobile': 161, 'burgundy': 162, 'messenger': 163, 'metallic': 164, 'eyes': 165, 'travel': 166, 'eyeshadow': 167, 'suspenders': 168, 'gloves': 169, 'camisoles': 170, 'salwar': 171, 'hair': 172, 'patiala': 173, 'jeggings': 174, 'bath': 175, 'stockings': 176, 'coffee': 177, 'churidar': 178, 'wash': 179, 'tracksuits': 180, 'mauve': 181, 'party': 182, 'cleanser': 183, 'rose': 184, 'sporting': 185, 'goods': 186, 'sunscreen': 187, 'robe': 188, 'nude': 189, 'sea': 190, 'equipment': 191, 'rain': 192, 'colour': 193, 'jacket': 194, 'water': 195, 'bottle': 196, 'swimwear': 197, 'waist': 198, 'baby': 199, 'dolls': 200, 'jumpsuit': 201, 'mushroom': 202, 'waistcoat': 203, 'basketballs': 204, 'mascara': 205, 'mask': 206, 'peel': 207, 'rompers': 208, 'booties': 209, 'umbrellas': 210, 'taupe': 211, 'wristbands': 212, 'concealer': 213, 'rucksacks': 214, 'tights': 215, 'shapewear': 216, 'blazers': 217, 'beauty': 218, 'footballs': 219, 'clothing': 220, 'headband': 221, 'shrug': 222, 'lime': 223, 'eye': 224, 'essentials': 225, 'scrub': 226, 'lotion': 227, 'perfumes': 228, 'exfoliator': 229, 'nehru': 230, 'toner': 231, 'fluorescent': 232, 'lehenga': 233, 'choli': 234, 'remover': 235, 'plumper': 236, 'trolley': 237, 'tablet': 238, 'sleeve': 239, 'home': 240, 'hat': 241, 'key': 242, 'chain': 243, 'serum': 244, 'gel': 245, 'laces': 246, 'furnishing': 247, 'cushion': 248, 'covers': 249, 'mens': 250, 'grooming': 251, 'kit': 252, 'vouchers': 253, 'ipad': 254}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqMJnykpTfax",
        "outputId": "2470e181-e925-433c-c4d5-823cf63f4867"
      },
      "source": [
        "da = {}\n",
        "ds = {}\n",
        "i=0\n",
        "a = tokenizer.sequences_to_matrix(train_sequences,mode='binary')\n",
        "print(a.shape)\n",
        "for k,v in data.items():\n",
        "  da[str(k)] = train_sequences[i]\n",
        "  i=i+1\n",
        "i=0\n",
        "for k,v in data.items():\n",
        "  ds[k] = np.array(a[i,:])\n",
        "  i=i+1\n",
        "print(da['10000'])\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy\n",
        "max_length = 13\n",
        "d_enc_padded = {}\n",
        "for k,d in da.items():\n",
        " pad = pad_sequences([d],maxlen=max_length,padding='post')\n",
        " d_enc_padded[str(k)] = numpy.array(pad[0])\n",
        "print(d_enc_padded['10000'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(44446, 240)\n",
            "[5, 3, 25, 128, 16, 4, 1]\n",
            "[  5   3  25 128  16   4   1   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDVqVI1ZTiuk"
      },
      "source": [
        "def read_image(fn):\n",
        "  im = cv2.imread(fn)\n",
        "  im = cv2.cvtColor(im, cv2.COLOR_BGR2LAB)\n",
        "  im = cv2.resize(im, (128,128), interpolation=cv2.INTER_CUBIC)\n",
        "  im = np.array(im)/255*2-1\n",
        "  return im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20pwI_fOTkr1",
        "outputId": "423d4584-cdb2-45f1-d907-0603f064aed2"
      },
      "source": [
        "traini = glob.glob(r'/content/drive/MyDrive/images/*')\n",
        "print(len(traini))\n",
        "def minibatch(data, batchsize):\n",
        "    length = len(data)\n",
        "    epoch = i = 0\n",
        "    tmpsize = None    \n",
        "    while True:\n",
        "        size = tmpsize if tmpsize else batchsize\n",
        "        if i+size > length:\n",
        "            shuffle(data)\n",
        "            i = 0\n",
        "            epoch+=1        \n",
        "        rtn = [read_image(data[j]) for j in range(i,i+size)]\n",
        "        ltn=[]\n",
        "        se = []\n",
        "        for j in range(i,i+size):\n",
        "          if '(' in data[j].split('/')[-1].split('.')[0]:\n",
        "            ltn.append(d_enc_padded[data[j].split('/')[-1].split('.')[0].split(' ')[0]])\n",
        "          else:\n",
        "            ltn.append(d_enc_padded[data[j].split('/')[-1].split('.')[0]])\n",
        "        i+=size\n",
        "        tmpsize = yield epoch, np.float32(rtn), np.array(ltn)/239\n",
        "\n",
        "def minibatchAB(dataA, batchsize):\n",
        "    batchA = minibatch(dataA, batchsize)\n",
        "    tmpsize = None    \n",
        "    while True:        \n",
        "        ep1, A,B = batchA.send(tmpsize)\n",
        "        tmpsize = yield ep1, A,B\n",
        "train_batch = minibatchAB(traini, 64)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz7Ecbqw9DzN"
      },
      "source": [
        "class ClipConstraint(tf.keras.constraints.Constraint):\n",
        "  def __init__(self, clip_value):\n",
        "    self.clip_value = clip_value\n",
        "  def __call__(self, weights):\n",
        "    return backend.clip(weights, -self.clip_value, self.clip_value)\n",
        "  def get_config(self):\n",
        "    return {'clip_value': self.clip_value}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLykj6zJ-CwL"
      },
      "source": [
        "const = ClipConstraint(0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcatxM4UTmXW"
      },
      "source": [
        "class CycleGAN():\n",
        "  def __init__(self,train_batch,const):\n",
        "    self.img_rows = 128 \n",
        "    self.img_cols = 128 \n",
        "    self.channels = 3 \n",
        "    self.traindata = train_batch\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "    self.label_shape = (13,)\n",
        "    self.lambda_cycle = 10.0 \n",
        "    optimizer = RMSprop(learning_rate =0.00005)\n",
        "    self.const = const\n",
        "    self.g_AB = self.build_label_generator() \n",
        "    self.g_BA = self.build_img_generator()\n",
        "    A = Input(shape=self.img_shape) \n",
        "    B = Input(shape=self.label_shape) \n",
        "    fake_B = self.g_AB(A) \n",
        "    fake_A = self.g_BA(B)\n",
        "    reconstr_A = self.g_BA(fake_B) \n",
        "    reconstr_B = self.g_AB(fake_A)\n",
        "    self.d_A = self.build_img_label_discriminator() \n",
        "    self.d_B = self.build_img_label_discriminator() \n",
        "    self.d_A.compile(loss='mse',optimizer=optimizer) \n",
        "    self.d_B.compile(loss='mse',optimizer=optimizer)  \n",
        "    self.d_A.trainable = False \n",
        "    self.d_B.trainable = False\n",
        "    valid_A = self.d_A([fake_A,B]) \n",
        "    valid_B = self.d_B([A,fake_B])\n",
        "    self.combined = Model(inputs=[A,B],outputs=[valid_A, valid_B,reconstr_A, reconstr_B]) \n",
        "    self.combined.compile(loss=[self.wa_loss,self.wa_loss,tf.keras.losses.kl_divergence,tf.keras.losses.kl_divergence],loss_weights=[1, 1,self.lambda_cycle, self.lambda_cycle],optimizer=optimizer)\n",
        "  def wa_loss(self,y_true, y_pred):\n",
        "    return K.mean(y_true * y_pred)\n",
        "  def build_img_generator(self):\n",
        "    d0 = Input(shape=self.label_shape)\n",
        "    d1 = Dense(128,activation = 'relu')(d0)\n",
        "    d2 = tf.keras.layers.Reshape((1,1,128))(d1)\n",
        "    d3 = Conv2DTranspose(128,kernel_size = 3,strides = 2,padding = 'same')(d2)\n",
        "    d3 = BatchNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(128,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = BatchNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(128,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = BatchNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(64,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = BatchNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(64,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = BatchNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(64,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = BatchNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(64,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = BatchNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(32,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(32,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(32,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = BatchNormalization()(d3)\n",
        "    d3 = LeakyReLU(alpha=0.2)(d3)\n",
        "    d3 = Conv2DTranspose(16,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(16,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(8,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    output_img = Conv2D(3,kernel_size = 3,strides = 1,padding = 'same',activation='tanh')(d3)\n",
        "    return Model(d0, output_img) \n",
        "  def build_label_generator(self):\n",
        "    d0 = Input(shape=self.img_shape)\n",
        "    d1 = Conv2D(32,kernel_size = 3,strides = 1,padding = 'same')(d0)\n",
        "    d1 = BatchNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(32,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = BatchNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(64,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = BatchNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(64,kernel_size = 3,strides = 1,padding = 'same')(d1)\n",
        "    d1 = BatchNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(64,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = BatchNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 1,padding = 'same')(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = BatchNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(256,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = BatchNormalization()(d1)\n",
        "    d1 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d1 = Conv2D(512,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d2 = Flatten()(d1)\n",
        "    d2 = Dense(512,activation='relu')(d2)\n",
        "    d2 = Dense(256,activation='relu')(d2)\n",
        "    d2 = Dense(128,activation='relu')(d2)\n",
        "    output_label = Dense(13,activation='relu')(d2)\n",
        "    return Model(d0, output_label)\n",
        "  def build_img_label_discriminator(self):\n",
        "    x1 = Input(shape=self.img_shape)\n",
        "    x2 = Input(shape=self.label_shape)\n",
        "    d1 = Conv2D(32,kernel_size = 3,strides = 4,kernel_constraint=self.const,padding = 'same')(x1)\n",
        "    d1 = Conv2D(64,kernel_size = 3,strides = 4,kernel_constraint=self.const,padding = 'same')(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 4,kernel_constraint=self.const,padding = 'same')(d1)\n",
        "    d1 = Conv2D(512,kernel_size = 3,strides = 2,kernel_constraint=self.const,padding = 'same')(d1)\n",
        "    d2 = Flatten()(d1)\n",
        "    d2 = Dense(13)(d2)\n",
        "    con = concatenate([d2,x2],axis=-1)\n",
        "    d3 = Dense(128,activation='relu')(con)\n",
        "    d3 = Dense(64,activation='relu')(d3)\n",
        "    validity = Dense(1)(d3)\n",
        "    return Model(inputs = [x1,x2], outputs = [validity])\n",
        "  def train(self, epochs, batch_size=64, sample_interval=50):\n",
        "    valid = np.ones((batch_size,1))*(-1)\n",
        "    fake = np.ones((batch_size,1))\n",
        "    for epoch in range(epochs):\n",
        "      for i in range(3):\n",
        "        ep, A,B= next(self.traindata)\n",
        "        fake_B = self.g_AB.predict(A) \n",
        "        fake_A = self.g_BA.predict(B)\n",
        "        dA_loss_real = self.d_A.train_on_batch([A,B], valid) \n",
        "        dA_loss_fake = self.d_A.train_on_batch([fake_A,B], fake)\n",
        "        dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "        dB_loss_real = self.d_B.train_on_batch([A,B], valid)\n",
        "        dB_loss_fake = self.d_B.train_on_batch([A,fake_B], fake)\n",
        "        dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "      d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
        "      ep, A,B= next(self.traindata)\n",
        "      g_loss = self.combined.train_on_batch([A,B],[valid, valid,A,B])\n",
        "      if epoch % sample_interval == 0: \n",
        "        print(\"%d [D loss : %f ] [G loss : %f]\"%(epoch,np.mean(d_loss),np.mean(g_loss)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRsqwq-_Tpin",
        "outputId": "a50182d4-5b42-41ca-b4fa-01a1400743f2"
      },
      "source": [
        "gan = CycleGAN(train_batch,const)\n",
        "gan.train(epochs=3000, batch_size=64, sample_interval=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [D loss : 1.000048 ] [G loss : 28.811867]\n",
            "1 [D loss : 0.997537 ] [G loss : 25.959620]\n",
            "2 [D loss : 0.995336 ] [G loss : 26.141638]\n",
            "3 [D loss : 0.993509 ] [G loss : 24.919598]\n",
            "4 [D loss : 0.991573 ] [G loss : 23.918114]\n",
            "5 [D loss : 0.991466 ] [G loss : 24.196969]\n",
            "6 [D loss : 0.988440 ] [G loss : 24.463877]\n",
            "7 [D loss : 0.986981 ] [G loss : 24.741200]\n",
            "8 [D loss : 0.983811 ] [G loss : 24.639153]\n",
            "9 [D loss : 0.982311 ] [G loss : 23.350153]\n",
            "10 [D loss : 0.977272 ] [G loss : 25.011986]\n",
            "11 [D loss : 0.972332 ] [G loss : 28.882066]\n",
            "12 [D loss : 0.954981 ] [G loss : 27.161019]\n",
            "13 [D loss : 0.944887 ] [G loss : 27.593961]\n",
            "14 [D loss : 0.924074 ] [G loss : 26.113601]\n",
            "15 [D loss : 0.917425 ] [G loss : 30.132301]\n",
            "16 [D loss : 0.890054 ] [G loss : 29.588988]\n",
            "17 [D loss : 0.870412 ] [G loss : 21.354742]\n",
            "18 [D loss : 0.851235 ] [G loss : 19.981461]\n",
            "19 [D loss : 0.806092 ] [G loss : 24.221764]\n",
            "20 [D loss : 0.779561 ] [G loss : 22.874282]\n",
            "21 [D loss : 0.757184 ] [G loss : 17.903221]\n",
            "22 [D loss : 0.740448 ] [G loss : 17.271977]\n",
            "23 [D loss : 0.720376 ] [G loss : 17.096713]\n",
            "24 [D loss : 0.709044 ] [G loss : 15.657939]\n",
            "25 [D loss : 0.694728 ] [G loss : 15.180837]\n",
            "26 [D loss : 0.687855 ] [G loss : 16.700681]\n",
            "27 [D loss : 0.683129 ] [G loss : 15.489490]\n",
            "28 [D loss : 0.671158 ] [G loss : 14.754869]\n",
            "29 [D loss : 0.669259 ] [G loss : 14.360201]\n",
            "30 [D loss : 0.650040 ] [G loss : 14.880233]\n",
            "31 [D loss : 0.677824 ] [G loss : 12.123430]\n",
            "32 [D loss : 0.653114 ] [G loss : 12.258129]\n",
            "33 [D loss : 0.625212 ] [G loss : 14.116025]\n",
            "34 [D loss : 0.619555 ] [G loss : 13.949207]\n",
            "35 [D loss : 0.618755 ] [G loss : 11.449902]\n",
            "36 [D loss : 0.605717 ] [G loss : 12.102384]\n",
            "37 [D loss : 0.599855 ] [G loss : 11.196399]\n",
            "38 [D loss : 0.586979 ] [G loss : 10.954887]\n",
            "39 [D loss : 0.563333 ] [G loss : 11.308480]\n",
            "40 [D loss : 0.549994 ] [G loss : 11.026350]\n",
            "41 [D loss : 0.528063 ] [G loss : 10.681480]\n",
            "42 [D loss : 0.511961 ] [G loss : 10.641547]\n",
            "43 [D loss : 0.504228 ] [G loss : 10.486496]\n",
            "44 [D loss : 0.497188 ] [G loss : 10.291440]\n",
            "45 [D loss : 0.479045 ] [G loss : 10.348080]\n",
            "46 [D loss : 0.482751 ] [G loss : 10.467524]\n",
            "47 [D loss : 0.457118 ] [G loss : 10.939575]\n",
            "48 [D loss : 0.461476 ] [G loss : 11.160792]\n",
            "49 [D loss : 0.467767 ] [G loss : 11.106318]\n",
            "50 [D loss : 0.454031 ] [G loss : 10.875085]\n",
            "51 [D loss : 0.475319 ] [G loss : 10.221311]\n",
            "52 [D loss : 0.471190 ] [G loss : 10.519626]\n",
            "53 [D loss : 0.438805 ] [G loss : 10.410401]\n",
            "54 [D loss : 0.453872 ] [G loss : 9.736998]\n",
            "55 [D loss : 0.471321 ] [G loss : 9.350505]\n",
            "56 [D loss : 0.460997 ] [G loss : 10.348964]\n",
            "57 [D loss : 0.472601 ] [G loss : 10.192711]\n",
            "58 [D loss : 0.456484 ] [G loss : 9.173938]\n",
            "59 [D loss : 0.453777 ] [G loss : 9.069791]\n",
            "60 [D loss : 0.463909 ] [G loss : 8.777885]\n",
            "61 [D loss : 0.451273 ] [G loss : 9.261807]\n",
            "62 [D loss : 0.460039 ] [G loss : 9.565977]\n",
            "63 [D loss : 0.453663 ] [G loss : 9.018718]\n",
            "64 [D loss : 0.454501 ] [G loss : 9.475157]\n",
            "65 [D loss : 0.456268 ] [G loss : 8.893050]\n",
            "66 [D loss : 0.453849 ] [G loss : 7.779549]\n",
            "67 [D loss : 0.453676 ] [G loss : 7.777397]\n",
            "68 [D loss : 0.455330 ] [G loss : 8.279576]\n",
            "69 [D loss : 0.449176 ] [G loss : 8.304309]\n",
            "70 [D loss : 0.453433 ] [G loss : 7.595043]\n",
            "71 [D loss : 0.457585 ] [G loss : 7.813500]\n",
            "72 [D loss : 0.452589 ] [G loss : 7.139187]\n",
            "73 [D loss : 0.445021 ] [G loss : 6.572961]\n",
            "74 [D loss : 0.446315 ] [G loss : 6.972570]\n",
            "75 [D loss : 0.438787 ] [G loss : 6.557130]\n",
            "76 [D loss : 0.441494 ] [G loss : 6.783452]\n",
            "77 [D loss : 0.443779 ] [G loss : 6.307741]\n",
            "78 [D loss : 0.427677 ] [G loss : 6.124329]\n",
            "79 [D loss : 0.432872 ] [G loss : 6.120073]\n",
            "80 [D loss : 0.436586 ] [G loss : 6.167895]\n",
            "81 [D loss : 0.434170 ] [G loss : 5.979504]\n",
            "82 [D loss : 0.442620 ] [G loss : 6.028416]\n",
            "83 [D loss : 0.433392 ] [G loss : 5.339836]\n",
            "84 [D loss : 0.434831 ] [G loss : 5.037273]\n",
            "85 [D loss : 0.433660 ] [G loss : 5.225514]\n",
            "86 [D loss : 0.424511 ] [G loss : 5.926627]\n",
            "87 [D loss : 0.425642 ] [G loss : 5.740490]\n",
            "88 [D loss : 0.433021 ] [G loss : 5.369014]\n",
            "89 [D loss : 0.435320 ] [G loss : 5.206995]\n",
            "90 [D loss : 0.424471 ] [G loss : 4.615757]\n",
            "91 [D loss : 0.416548 ] [G loss : 4.323360]\n",
            "92 [D loss : 0.409977 ] [G loss : 4.719115]\n",
            "93 [D loss : 0.413281 ] [G loss : 4.204648]\n",
            "94 [D loss : 0.415471 ] [G loss : 4.616385]\n",
            "95 [D loss : 0.426238 ] [G loss : 4.147183]\n",
            "96 [D loss : 0.412099 ] [G loss : 3.956862]\n",
            "97 [D loss : 0.404136 ] [G loss : 4.258277]\n",
            "98 [D loss : 0.424417 ] [G loss : 3.512937]\n",
            "99 [D loss : 0.415877 ] [G loss : 3.983694]\n",
            "100 [D loss : 0.423989 ] [G loss : 3.923603]\n",
            "101 [D loss : 0.398763 ] [G loss : 3.835462]\n",
            "102 [D loss : 0.404757 ] [G loss : 3.707547]\n",
            "103 [D loss : 0.391719 ] [G loss : 3.483735]\n",
            "104 [D loss : 0.418561 ] [G loss : 3.037100]\n",
            "105 [D loss : 0.388512 ] [G loss : 3.193917]\n",
            "106 [D loss : 0.409841 ] [G loss : 3.039056]\n",
            "107 [D loss : 0.408272 ] [G loss : 2.576492]\n",
            "108 [D loss : 0.396566 ] [G loss : 3.169057]\n",
            "109 [D loss : 0.387913 ] [G loss : 2.371306]\n",
            "110 [D loss : 0.390247 ] [G loss : 2.737960]\n",
            "111 [D loss : 0.402267 ] [G loss : 3.124595]\n",
            "112 [D loss : 0.394346 ] [G loss : 2.734711]\n",
            "113 [D loss : 0.394157 ] [G loss : 2.842495]\n",
            "114 [D loss : 0.384663 ] [G loss : 3.020142]\n",
            "115 [D loss : 0.387964 ] [G loss : 1.997486]\n",
            "116 [D loss : 0.384104 ] [G loss : 2.171894]\n",
            "117 [D loss : 0.373290 ] [G loss : 2.998762]\n",
            "118 [D loss : 0.374562 ] [G loss : 3.074346]\n",
            "119 [D loss : 0.394585 ] [G loss : 2.630423]\n",
            "120 [D loss : 0.378188 ] [G loss : 2.465385]\n",
            "121 [D loss : 0.397159 ] [G loss : 2.315827]\n",
            "122 [D loss : 0.382553 ] [G loss : 1.725873]\n",
            "123 [D loss : 0.373571 ] [G loss : 1.902307]\n",
            "124 [D loss : 0.371045 ] [G loss : 1.925993]\n",
            "125 [D loss : 0.369398 ] [G loss : 2.129485]\n",
            "126 [D loss : 0.392978 ] [G loss : 2.129846]\n",
            "127 [D loss : 0.376220 ] [G loss : 1.892913]\n",
            "128 [D loss : 0.377310 ] [G loss : 1.898582]\n",
            "129 [D loss : 0.378211 ] [G loss : 2.080110]\n",
            "130 [D loss : 0.376890 ] [G loss : 1.835217]\n",
            "131 [D loss : 0.379882 ] [G loss : 2.236586]\n",
            "132 [D loss : 0.369582 ] [G loss : 2.227812]\n",
            "133 [D loss : 0.361433 ] [G loss : 1.305531]\n",
            "134 [D loss : 0.375891 ] [G loss : 1.530143]\n",
            "135 [D loss : 0.376513 ] [G loss : 1.677676]\n",
            "136 [D loss : 0.388440 ] [G loss : 1.572759]\n",
            "137 [D loss : 0.358418 ] [G loss : 1.428807]\n",
            "138 [D loss : 0.357527 ] [G loss : 1.171515]\n",
            "139 [D loss : 0.349470 ] [G loss : 1.193104]\n",
            "140 [D loss : 0.361296 ] [G loss : 1.224968]\n",
            "141 [D loss : 0.357979 ] [G loss : 1.482528]\n",
            "142 [D loss : 0.354431 ] [G loss : 1.155010]\n",
            "143 [D loss : 0.351168 ] [G loss : 1.058578]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbJNS-v9TqHt"
      },
      "source": [
        "A = read_image('/content/drive/MyDrive/images/10000.jpg')\n",
        "A = np.expand_dims(A,axis=0)\n",
        "res = gan.g_AB.predict(A)\n",
        "print(d_enc_padded['10000'])\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2n3S3pERbIV"
      },
      "source": [
        "gan.save('/content/drive/MyDrive/cwgan.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}