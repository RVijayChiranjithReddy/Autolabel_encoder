{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoLabelEncoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMajoVuJSboXx+3nWQ0HIZ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RVijayChiranjithReddy/Autolabel_encoder/blob/main/AutoLabelEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBY-Q2146bt5",
        "outputId": "93115ae1-61f7-4027-f231-6b9fe220f27a"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import *\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.activations import *\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.applications import *\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import glob\n",
        "from random import randint, shuffle\n",
        "%matplotlib inline\n",
        "K.set_learning_phase(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN55xuWH9_qE",
        "outputId": "14e9c74f-436d-4d64-d02f-f1210204b15a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CQgv4Sw-t5v",
        "outputId": "ac50d2b0-26fc-4bad-9ced-26d916fc68b6"
      },
      "source": [
        "import pandas\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "df = pandas.read_csv('/content/styles.csv')\n",
        "df.fillna('DK', inplace = True) \n",
        "data = {}\n",
        "for i in range(44446):\n",
        "  tags = [str(x) for x in df.loc[i].values[1:9] if type(x) is str]\n",
        "  #desc = df.loc[i].values[9]\n",
        "  #if type(desc) is not float:\n",
        "    #for de in desc.split():\n",
        "     #if de.isalpha() and de not in tags:\n",
        "        #tags.append(de)\n",
        "  data[str(df.loc[i].values[0])] = ':'.join(tags)\n",
        "vocab = []\n",
        "for k,v in data.items():\n",
        "  temp = list(set(v.split(':')))\n",
        "  vocab = vocab + temp\n",
        "  vocab = list(set(vocab))\n",
        "print(len(vocab))\n",
        "#from tensorflow.keras.preprocessing.text import one_hot\n",
        "#for k,d in data.items():\n",
        "  #data_encoded[k] = one_hot(' '.join(d),len(vocab))\n",
        "tokenizer = Tokenizer(num_words=len(vocab))\n",
        "\n",
        "# This builds the word index\n",
        "tokenizer.fit_on_texts(data.values())\n",
        "\n",
        "\n",
        "# This turns strings into lists of integer indices.\n",
        "train_sequences = tokenizer.texts_to_sequences(data.values())\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "240\n",
            "{'casual': 1, 'men': 2, 'apparel': 3, 'summer': 4, 'women': 5, 'topwear': 6, 'shoes': 7, 'accessories': 8, 'fall': 9, 'black': 10, 'footwear': 11, 'winter': 12, 'tshirts': 13, 'blue': 14, 'sports': 15, 'white': 16, 'watches': 17, 'brown': 18, 'shirts': 19, 'ethnic': 20, 'bags': 21, 'formal': 22, 'spring': 23, 'grey': 24, 'bottomwear': 25, 'care': 26, 'red': 27, 'personal': 28, 'unisex': 29, 'green': 30, 'innerwear': 31, 'wallets': 32, 'pink': 33, 'kurtas': 34, 'flip': 35, 'flops': 36, 'navy': 37, 'tops': 38, 'handbags': 39, 'purple': 40, 'belts': 41, 'and': 42, 'socks': 43, 'heels': 44, 'jewellery': 45, 'silver': 46, 'eyewear': 47, 'sunglasses': 48, 'fragrance': 49, 'sandals': 50, 'sandal': 51, 'briefs': 52, 'boys': 53, 'yellow': 54, 'beige': 55, 'backpacks': 56, 'girls': 57, 'body': 58, 'gold': 59, 'perfume': 60, 'mist': 61, 'jeans': 62, 'shorts': 63, 'maroon': 64, 'trousers': 65, 'orange': 66, 'lips': 67, 'ties': 68, 'flats': 69, 'dress': 70, 'bra': 71, 'loungewear': 72, 'nightwear': 73, 'dresses': 74, 'saree': 75, 'sarees': 76, 'earrings': 77, 'olive': 78, 'cream': 79, 'multi': 80, 'pants': 81, 'dk': 82, 'deodorant': 83, 'set': 84, 'nail': 85, 'nails': 86, 'polish': 87, 'skin': 88, 'lipstick': 89, 'steel': 90, 'makeup': 91, 'track': 92, 'free': 93, 'headwear': 94, 'clutches': 95, 'sweatshirts': 96, 'caps': 97, 'sweaters': 98, 'jackets': 99, 'vests': 100, 'scarves': 101, 'kurtis': 102, 'tunics': 103, 'charcoal': 104, 'bag': 105, 'cufflinks': 106, 'lip': 107, 'peach': 108, 'gifts': 109, 'nightdress': 110, 'off': 111, 'stoles': 112, 'leggings': 113, 'pendant': 114, 'capris': 115, 'gift': 116, 'lavender': 117, 'necklace': 118, 'chains': 119, 'melange': 120, 'gloss': 121, 'suits': 122, 'night': 123, 'trunk': 124, 'khaki': 125, 'accessory': 126, 'magenta': 127, 'skirts': 128, 'dupatta': 129, 'teal': 130, 'ring': 131, 'tan': 132, 'items': 133, 'kajal': 134, 'eyeliner': 135, 'lounge': 136, 'mustard': 137, 'face': 138, 'bronze': 139, 'kurta': 140, 'sets': 141, 'duffel': 142, 'copper': 143, 'bangle': 144, 'laptop': 145, 'mufflers': 146, 'foundation': 147, 'primer': 148, 'turquoise': 149, 'smart': 150, 'bracelet': 151, 'rust': 152, 'pouch': 153, 'moisturisers': 154, 'highlighter': 155, 'blush': 156, 'boxers': 157, 'compact': 158, 'shoe': 159, 'liner': 160, 'mobile': 161, 'burgundy': 162, 'messenger': 163, 'metallic': 164, 'eyes': 165, 'travel': 166, 'eyeshadow': 167, 'suspenders': 168, 'gloves': 169, 'camisoles': 170, 'salwar': 171, 'hair': 172, 'patiala': 173, 'jeggings': 174, 'bath': 175, 'stockings': 176, 'coffee': 177, 'churidar': 178, 'wash': 179, 'tracksuits': 180, 'mauve': 181, 'party': 182, 'cleanser': 183, 'rose': 184, 'sporting': 185, 'goods': 186, 'sunscreen': 187, 'robe': 188, 'nude': 189, 'sea': 190, 'equipment': 191, 'rain': 192, 'colour': 193, 'jacket': 194, 'water': 195, 'bottle': 196, 'swimwear': 197, 'waist': 198, 'baby': 199, 'dolls': 200, 'jumpsuit': 201, 'mushroom': 202, 'waistcoat': 203, 'basketballs': 204, 'mascara': 205, 'mask': 206, 'peel': 207, 'rompers': 208, 'booties': 209, 'umbrellas': 210, 'taupe': 211, 'wristbands': 212, 'concealer': 213, 'rucksacks': 214, 'tights': 215, 'shapewear': 216, 'blazers': 217, 'beauty': 218, 'footballs': 219, 'clothing': 220, 'headband': 221, 'shrug': 222, 'lime': 223, 'eye': 224, 'essentials': 225, 'scrub': 226, 'lotion': 227, 'perfumes': 228, 'exfoliator': 229, 'nehru': 230, 'toner': 231, 'fluorescent': 232, 'lehenga': 233, 'choli': 234, 'remover': 235, 'plumper': 236, 'trolley': 237, 'tablet': 238, 'sleeve': 239, 'home': 240, 'hat': 241, 'key': 242, 'chain': 243, 'serum': 244, 'gel': 245, 'laces': 246, 'furnishing': 247, 'cushion': 248, 'covers': 249, 'mens': 250, 'grooming': 251, 'kit': 252, 'vouchers': 253, 'ipad': 254}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgd-zGda-2RI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d7e5b6f-c231-4485-bfd2-6cec96259ac0"
      },
      "source": [
        "da = {}\n",
        "ds = {}\n",
        "i=0\n",
        "a = tokenizer.sequences_to_matrix(train_sequences,mode='binary')\n",
        "print(a.shape)\n",
        "for k,v in data.items():\n",
        "  da[str(k)] = train_sequences[i]\n",
        "  i=i+1\n",
        "i=0\n",
        "for k,v in data.items():\n",
        "  ds[k] = np.array(a[i,:])\n",
        "  i=i+1\n",
        "print(da['10000'])\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy\n",
        "max_length = 13\n",
        "d_enc_padded = {}\n",
        "for k,d in da.items():\n",
        " pad = pad_sequences([d],maxlen=max_length,padding='post')\n",
        " d_enc_padded[str(k)] = numpy.array(pad[0])\n",
        "print(d_enc_padded['10000'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(44446, 240)\n",
            "[5, 3, 25, 128, 16, 4, 1]\n",
            "[  5   3  25 128  16   4   1   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ikaeitI-5Sg"
      },
      "source": [
        "def read_image(fn):\n",
        "  im = cv2.imread(fn)\n",
        "  im = cv2.cvtColor(im, cv2.COLOR_BGR2LAB)\n",
        "  im = cv2.resize(im, (128,128), interpolation=cv2.INTER_CUBIC)\n",
        "  im = np.array(im)/255*2-1\n",
        "  return im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSByZ11cJGVs",
        "outputId": "cb6dcf4b-0136-45c2-e65f-50423c724819"
      },
      "source": [
        "traini = glob.glob(r'/content/drive/MyDrive/images/*')\n",
        "print(len(traini))\n",
        "def minibatch(data, batchsize):\n",
        "    length = len(data)\n",
        "    epoch = i = 0\n",
        "    tmpsize = None    \n",
        "    while True:\n",
        "        size = tmpsize if tmpsize else batchsize\n",
        "        if i+size > length:\n",
        "            shuffle(data)\n",
        "            i = 0\n",
        "            epoch+=1        \n",
        "        rtn = [read_image(data[j]) for j in range(i,i+size)]\n",
        "        ltn=[]\n",
        "        se = []\n",
        "        for j in range(i,i+size):\n",
        "          if '(' in data[j].split('/')[-1].split('.')[0]:\n",
        "            ltn.append(d_enc_padded[data[j].split('/')[-1].split('.')[0].split(' ')[0]])\n",
        "          else:\n",
        "            ltn.append(d_enc_padded[data[j].split('/')[-1].split('.')[0]])\n",
        "        i+=size\n",
        "        tmpsize = yield epoch, np.float32(rtn), np.array(ltn)\n",
        "\n",
        "def minibatchAB(dataA, batchsize):\n",
        "    batchA = minibatch(dataA, batchsize)\n",
        "    tmpsize = None    \n",
        "    while True:        \n",
        "        ep1, A,B = batchA.send(tmpsize)\n",
        "        tmpsize = yield ep1, A,B\n",
        "train_batch = minibatchAB(traini, 1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3mSkZwcJEh3"
      },
      "source": [
        "class CycleGAN():\n",
        "  def __init__(self,train_batch):\n",
        "    self.img_rows = 128 \n",
        "    self.img_cols = 128 \n",
        "    self.channels = 3 \n",
        "    self.traindata = train_batch\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "    self.label_shape = (13,)\n",
        "    self.lambda_cycle = 10.0 \n",
        "    optimizer = Adam(0.001, 0.5)\n",
        "    self.g_AB = self.build_label_generator() \n",
        "    self.g_BA = self.build_img_generator()\n",
        "    A = Input(shape=self.img_shape) \n",
        "    B = Input(shape=self.label_shape) \n",
        "    fake_B = self.g_AB(A) \n",
        "    fake_A = self.g_BA(B)\n",
        "    reconstr_A = self.g_BA(fake_B) \n",
        "    reconstr_B = self.g_AB(fake_A)\n",
        "    self.d_A = self.build_img_label_discriminator() \n",
        "    self.d_B = self.build_img_label_discriminator() \n",
        "    #self.d_A.compile(loss='mse',optimizer=optimizer,metrics=['accuracy']) \n",
        "    #self.d_B.compile(loss='mse',optimizer=optimizer, metrics=['accuracy'])  \n",
        "    self.d_A.trainable = False \n",
        "    self.d_B.trainable = False\n",
        "    valid_A = self.d_A([fake_A,B]) \n",
        "    valid_B = self.d_B([A,fake_B])\n",
        "    self.combined = Model(inputs=[A,B],outputs=[valid_A, valid_B,reconstr_A, reconstr_B]) \n",
        "    #self.combined.compile(loss=['binary_crossentropy','binary_crossentropy','mae','mae'],loss_weights=[1, 1,self.lambda_cycle, self.lambda_cycle],optimizer=optimizer)\n",
        "  def build_img_generator(self):\n",
        "    d0 = Input(shape=self.label_shape)\n",
        "    d1 = Dense(128,activation = 'relu')(d0)\n",
        "    d2 = tf.keras.layers.Reshape((1,1,128))(d1)\n",
        "    d3 = Conv2DTranspose(128,kernel_size = 3,strides = 2,padding = 'same')(d2)\n",
        "    d3 = Conv2DTranspose(128,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(128,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(64,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(64,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(64,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(64,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(32,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(32,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(32,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(16,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(16,kernel_size = 3,strides = 2,padding = 'same')(d3)\n",
        "    d3 = Conv2DTranspose(8,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    output_img = Conv2D(3,kernel_size = 3,strides = 1,padding = 'same')(d3)\n",
        "    return Model(d0, output_img) \n",
        "  def build_label_generator(self):\n",
        "    d0 = Input(shape=self.img_shape)\n",
        "    d1 = Conv2D(32,kernel_size = 3,strides = 1,padding = 'same')(d0)\n",
        "    d1 = Conv2D(32,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = Conv2D(64,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = Conv2D(64,kernel_size = 3,strides = 1,padding = 'same')(d1)\n",
        "    d1 = Conv2D(64,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 1,padding = 'same')(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = Conv2D(256,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d1 = Conv2D(512,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d2 = Flatten()(d1)\n",
        "    d2 = Dense(512,activation='relu')(d2)\n",
        "    d2 = Dense(256,activation='relu')(d2)\n",
        "    d2 = Dense(128,activation='relu')(d2)\n",
        "    output_label = Dense(13,activation='relu')(d2)\n",
        "    return Model(d0, output_label)\n",
        "  def build_img_label_discriminator(self):\n",
        "    x1 = Input(shape=self.img_shape)\n",
        "    x2 = Input(shape=self.label_shape)\n",
        "    d1 = Conv2D(32,kernel_size = 3,strides = 4,padding = 'same')(x1)\n",
        "    d1 = Conv2D(64,kernel_size = 3,strides = 4,padding = 'same')(d1)\n",
        "    d1 = Conv2D(128,kernel_size = 3,strides = 4,padding = 'same')(d1)\n",
        "    d1 = Conv2D(512,kernel_size = 3,strides = 2,padding = 'same')(d1)\n",
        "    d2 = Flatten()(d1)\n",
        "    d2 = Dense(13)(d2)\n",
        "    con = concatenate([d2,x2],axis=-1)\n",
        "    d3 = Dense(128,activation='relu')(con)\n",
        "    d3 = Dense(64,activation='relu')(d3)\n",
        "    validity = Dense(1,activation = 'tanh')(d3)\n",
        "    return Model(inputs = [x1,x2], outputs = validity)\n",
        "  def train(self, epochs, batch_size=64, sample_interval=50):\n",
        "    valid = np.ones((batch_size,1)) \n",
        "    fake = np.ones((batch_size,1))*(-1)\n",
        "    for epoch in range(epochs):\n",
        "      ep, A,B= next(self.traindata)\n",
        "      fake_B = self.g_AB.predict(A) \n",
        "      fake_A = self.g_BA.predict(B)\n",
        "      dA_loss_real = self.d_A.train_on_batch([A,B], valid) \n",
        "      dA_loss_fake = self.d_A.train_on_batch([fake_A,B], fake)\n",
        "      dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "      dB_loss_real = self.d_B.train_on_batch([A,B], valid)\n",
        "      dB_loss_fake = self.d_B.train_on_batch([A,fake_B], fake)\n",
        "      dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "      d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
        "      g_loss = self.combined.train_on_batch([A,B],[valid, valid,A,B])\n",
        "      if epoch % sample_interval == 0: \n",
        "        print(\"%d [D loss : %f ] [G loss : %f]\"%(epoch,np.mean(d_loss),np.mean(g_loss))) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtnJAWGeTrvu",
        "outputId": "45229e26-5731-4fb0-8518-e6e53310fd78"
      },
      "source": [
        "gan = CycleGAN(train_batch)\n",
        "gan.train(epochs=3000, batch_size=1, sample_interval=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 2255 calls to <function Model.make_train_function.<locals>.train_function at 0x7fe1e5cef560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 2257 calls to <function Model.make_train_function.<locals>.train_function at 0x7fe1e06d5440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0 [D loss : 0.354042 ] [G loss : 9.125847]\n",
            "10 [D loss : 0.510293 ] [G loss : 24.149992]\n",
            "20 [D loss : 0.500000 ] [G loss : 615.203426]\n",
            "30 [D loss : 0.500000 ] [G loss : 101.361020]\n",
            "40 [D loss : 0.500000 ] [G loss : 117.727549]\n",
            "50 [D loss : 0.500000 ] [G loss : 92.869829]\n",
            "60 [D loss : 0.500000 ] [G loss : 73.017120]\n",
            "70 [D loss : 0.500000 ] [G loss : 48.366384]\n",
            "80 [D loss : 0.500000 ] [G loss : 54.253828]\n",
            "90 [D loss : 0.500000 ] [G loss : 56.957403]\n",
            "100 [D loss : 0.500000 ] [G loss : 34.751327]\n",
            "110 [D loss : 0.500000 ] [G loss : 62.023795]\n",
            "120 [D loss : 0.500000 ] [G loss : 64.315420]\n",
            "130 [D loss : 0.500000 ] [G loss : 51.110827]\n",
            "140 [D loss : 0.500000 ] [G loss : 65.989373]\n",
            "150 [D loss : 0.500000 ] [G loss : 71.811690]\n",
            "160 [D loss : 0.500000 ] [G loss : 49.245679]\n",
            "170 [D loss : 0.500000 ] [G loss : 49.185560]\n",
            "180 [D loss : 0.500000 ] [G loss : 80.543898]\n",
            "190 [D loss : 0.500000 ] [G loss : 49.402650]\n",
            "200 [D loss : 0.500000 ] [G loss : 83.654140]\n",
            "210 [D loss : 0.500000 ] [G loss : 72.428125]\n",
            "220 [D loss : 0.500000 ] [G loss : 40.492056]\n",
            "230 [D loss : 0.500000 ] [G loss : 58.582217]\n",
            "240 [D loss : 0.500000 ] [G loss : 60.602062]\n",
            "250 [D loss : 0.500000 ] [G loss : 84.982795]\n",
            "260 [D loss : 0.500000 ] [G loss : 37.388875]\n",
            "270 [D loss : 0.500000 ] [G loss : 117.001808]\n",
            "280 [D loss : 0.500000 ] [G loss : 51.760444]\n",
            "290 [D loss : 0.500000 ] [G loss : 57.444553]\n",
            "300 [D loss : 0.500000 ] [G loss : 85.141828]\n",
            "310 [D loss : 0.500000 ] [G loss : 128.466919]\n",
            "320 [D loss : 0.500000 ] [G loss : 126.701617]\n",
            "330 [D loss : 0.500000 ] [G loss : 125.265931]\n",
            "340 [D loss : 0.500000 ] [G loss : 97.899305]\n",
            "350 [D loss : 0.500000 ] [G loss : 71.714025]\n",
            "360 [D loss : 0.500000 ] [G loss : 180.622855]\n",
            "370 [D loss : 0.500000 ] [G loss : 86.217421]\n",
            "380 [D loss : 0.500000 ] [G loss : 442.884472]\n",
            "390 [D loss : 0.500000 ] [G loss : 144.214959]\n",
            "400 [D loss : 0.500000 ] [G loss : 38.084005]\n",
            "410 [D loss : 0.500000 ] [G loss : 10.719154]\n",
            "420 [D loss : 0.500000 ] [G loss : 15.626323]\n",
            "430 [D loss : 0.500000 ] [G loss : 11.071366]\n",
            "440 [D loss : 0.500000 ] [G loss : 15.512793]\n",
            "450 [D loss : 0.500000 ] [G loss : 13.131459]\n",
            "460 [D loss : 0.500000 ] [G loss : 10.790081]\n",
            "470 [D loss : 0.500000 ] [G loss : 35.609479]\n",
            "480 [D loss : 0.500000 ] [G loss : 11.252734]\n",
            "490 [D loss : 0.500000 ] [G loss : 14.987235]\n",
            "500 [D loss : 0.500000 ] [G loss : 26.908863]\n",
            "510 [D loss : 0.500000 ] [G loss : 25.448848]\n",
            "520 [D loss : 0.500000 ] [G loss : 20.187063]\n",
            "530 [D loss : 0.500000 ] [G loss : 15.298099]\n",
            "540 [D loss : 0.500000 ] [G loss : 21.066037]\n",
            "550 [D loss : 0.500000 ] [G loss : 21.151538]\n",
            "560 [D loss : 0.500000 ] [G loss : 9.649330]\n",
            "570 [D loss : 0.500000 ] [G loss : 11.768046]\n",
            "580 [D loss : 0.500000 ] [G loss : 15.050080]\n",
            "590 [D loss : 0.500000 ] [G loss : 16.932900]\n",
            "600 [D loss : 0.500000 ] [G loss : 23.418153]\n",
            "610 [D loss : 0.500000 ] [G loss : 25.698726]\n",
            "620 [D loss : 0.500000 ] [G loss : 11.985640]\n",
            "630 [D loss : 0.500000 ] [G loss : 25.770598]\n",
            "640 [D loss : 0.500000 ] [G loss : 10.409541]\n",
            "650 [D loss : 0.500000 ] [G loss : 28.163384]\n",
            "660 [D loss : 0.500000 ] [G loss : 11.704375]\n",
            "670 [D loss : 0.500000 ] [G loss : 11.392509]\n",
            "680 [D loss : 0.500000 ] [G loss : 9.846783]\n",
            "690 [D loss : 0.500000 ] [G loss : 9.284233]\n",
            "700 [D loss : 0.500000 ] [G loss : 9.786014]\n",
            "710 [D loss : 0.500000 ] [G loss : 16.374105]\n",
            "720 [D loss : 0.500000 ] [G loss : 17.526597]\n",
            "730 [D loss : 0.500000 ] [G loss : 25.376600]\n",
            "740 [D loss : 0.500000 ] [G loss : 9.256422]\n",
            "750 [D loss : 0.500000 ] [G loss : 13.152499]\n",
            "760 [D loss : 0.500000 ] [G loss : 23.087177]\n",
            "770 [D loss : 0.500000 ] [G loss : 29.103161]\n",
            "780 [D loss : 0.500000 ] [G loss : 23.879141]\n",
            "790 [D loss : 0.500000 ] [G loss : 8.660726]\n",
            "800 [D loss : 0.500000 ] [G loss : 24.382961]\n",
            "810 [D loss : 0.500000 ] [G loss : 8.607464]\n",
            "820 [D loss : 0.500000 ] [G loss : 16.087337]\n",
            "830 [D loss : 0.500000 ] [G loss : 9.845277]\n",
            "840 [D loss : 0.500000 ] [G loss : 13.493836]\n",
            "850 [D loss : 0.500000 ] [G loss : 9.249105]\n",
            "860 [D loss : 0.500000 ] [G loss : 23.634278]\n",
            "870 [D loss : 0.500000 ] [G loss : 10.049968]\n",
            "880 [D loss : 0.500000 ] [G loss : 12.875916]\n",
            "890 [D loss : 0.500000 ] [G loss : 25.203060]\n",
            "900 [D loss : 0.500000 ] [G loss : 15.505576]\n",
            "910 [D loss : 0.500000 ] [G loss : 23.647860]\n",
            "920 [D loss : 0.500000 ] [G loss : 12.973055]\n",
            "930 [D loss : 0.500000 ] [G loss : 33.145137]\n",
            "940 [D loss : 0.500000 ] [G loss : 9.366086]\n",
            "950 [D loss : 0.500000 ] [G loss : 44.802300]\n",
            "960 [D loss : 0.500000 ] [G loss : 21.483255]\n",
            "970 [D loss : 0.500000 ] [G loss : 25.229780]\n",
            "980 [D loss : 0.500000 ] [G loss : 20.938576]\n",
            "990 [D loss : 0.500000 ] [G loss : 12.433406]\n",
            "1000 [D loss : 0.500000 ] [G loss : 13.817572]\n",
            "1010 [D loss : 0.500000 ] [G loss : 31.317646]\n",
            "1020 [D loss : 0.500000 ] [G loss : 11.930066]\n",
            "1030 [D loss : 0.500000 ] [G loss : 14.581852]\n",
            "1040 [D loss : 0.500000 ] [G loss : 15.530575]\n",
            "1050 [D loss : 0.500000 ] [G loss : 19.602730]\n",
            "1060 [D loss : 0.500000 ] [G loss : 10.767398]\n",
            "1070 [D loss : 0.500000 ] [G loss : 12.945486]\n",
            "1080 [D loss : 0.500000 ] [G loss : 9.595933]\n",
            "1090 [D loss : 0.500000 ] [G loss : 14.013193]\n",
            "1100 [D loss : 0.500000 ] [G loss : 15.011463]\n",
            "1110 [D loss : 0.500000 ] [G loss : 18.387016]\n",
            "1120 [D loss : 0.500000 ] [G loss : 19.740581]\n",
            "1130 [D loss : 0.500000 ] [G loss : 12.861688]\n",
            "1140 [D loss : 0.500000 ] [G loss : 12.948036]\n",
            "1150 [D loss : 0.500000 ] [G loss : 23.908259]\n",
            "1160 [D loss : 0.500000 ] [G loss : 21.203566]\n",
            "1170 [D loss : 0.500000 ] [G loss : 13.745836]\n",
            "1180 [D loss : 0.500000 ] [G loss : 11.484685]\n",
            "1190 [D loss : 0.500000 ] [G loss : 22.352621]\n",
            "1200 [D loss : 0.500000 ] [G loss : 16.359781]\n",
            "1210 [D loss : 0.500000 ] [G loss : 10.865755]\n",
            "1220 [D loss : 0.500000 ] [G loss : 18.864454]\n",
            "1230 [D loss : 0.500000 ] [G loss : 18.171852]\n",
            "1240 [D loss : 0.500000 ] [G loss : 10.132544]\n",
            "1250 [D loss : 0.500000 ] [G loss : 9.885433]\n",
            "1260 [D loss : 0.500000 ] [G loss : 9.262228]\n",
            "1270 [D loss : 0.500000 ] [G loss : 12.302755]\n",
            "1280 [D loss : 0.500000 ] [G loss : 10.105459]\n",
            "1290 [D loss : 0.500000 ] [G loss : 28.660281]\n",
            "1300 [D loss : 0.500000 ] [G loss : 10.150688]\n",
            "1310 [D loss : 0.500000 ] [G loss : 26.154120]\n",
            "1320 [D loss : 0.500000 ] [G loss : 11.879245]\n",
            "1330 [D loss : 0.500000 ] [G loss : 14.210884]\n",
            "1340 [D loss : 0.500000 ] [G loss : 15.865556]\n",
            "1350 [D loss : 0.500000 ] [G loss : 14.024484]\n",
            "1360 [D loss : 0.500000 ] [G loss : 10.798247]\n",
            "1370 [D loss : 0.500000 ] [G loss : 10.156257]\n",
            "1380 [D loss : 0.500000 ] [G loss : 22.192517]\n",
            "1390 [D loss : 0.500000 ] [G loss : 14.503247]\n",
            "1400 [D loss : 0.500000 ] [G loss : 18.859787]\n",
            "1410 [D loss : 0.500000 ] [G loss : 14.257535]\n",
            "1420 [D loss : 0.500000 ] [G loss : 14.038525]\n",
            "1430 [D loss : 0.500000 ] [G loss : 10.963279]\n",
            "1440 [D loss : 0.500000 ] [G loss : 230.428236]\n",
            "1450 [D loss : 0.500000 ] [G loss : 23.649230]\n",
            "1460 [D loss : 0.500000 ] [G loss : 13.944585]\n",
            "1470 [D loss : 0.500000 ] [G loss : 22.297397]\n",
            "1480 [D loss : 0.500000 ] [G loss : 16.782728]\n",
            "1490 [D loss : 0.500000 ] [G loss : 9.483388]\n",
            "1500 [D loss : 0.500000 ] [G loss : 28.113130]\n",
            "1510 [D loss : 0.500000 ] [G loss : 11.852659]\n",
            "1520 [D loss : 0.500000 ] [G loss : 27.463015]\n",
            "1530 [D loss : 0.500000 ] [G loss : 10.168875]\n",
            "1540 [D loss : 0.500000 ] [G loss : 11.013234]\n",
            "1550 [D loss : 0.500000 ] [G loss : 9.061159]\n",
            "1560 [D loss : 0.500000 ] [G loss : 22.860528]\n",
            "1570 [D loss : 0.500000 ] [G loss : 10.666844]\n",
            "1580 [D loss : 0.500000 ] [G loss : 9.159252]\n",
            "1590 [D loss : 0.500000 ] [G loss : 9.041141]\n",
            "1600 [D loss : 0.500000 ] [G loss : 13.320687]\n",
            "1610 [D loss : 0.500000 ] [G loss : 10.047058]\n",
            "1620 [D loss : 0.500000 ] [G loss : 16.261800]\n",
            "1630 [D loss : 0.500000 ] [G loss : 24.236248]\n",
            "1640 [D loss : 0.500000 ] [G loss : 14.537162]\n",
            "1650 [D loss : 0.500000 ] [G loss : 26.546018]\n",
            "1660 [D loss : 0.500000 ] [G loss : 12.030577]\n",
            "1670 [D loss : 0.500000 ] [G loss : 18.937141]\n",
            "1680 [D loss : 0.500000 ] [G loss : 16.089289]\n",
            "1690 [D loss : 0.500000 ] [G loss : 13.347024]\n",
            "1700 [D loss : 0.500000 ] [G loss : 20.458046]\n",
            "1710 [D loss : 0.500000 ] [G loss : 22.703266]\n",
            "1720 [D loss : 0.500000 ] [G loss : 41.051420]\n",
            "1730 [D loss : 0.500000 ] [G loss : 21.630026]\n",
            "1740 [D loss : 0.500000 ] [G loss : 39.871391]\n",
            "1750 [D loss : 0.500000 ] [G loss : 14.226659]\n",
            "1760 [D loss : 0.500000 ] [G loss : 10.301621]\n",
            "1770 [D loss : 0.500000 ] [G loss : 20.409806]\n",
            "1780 [D loss : 0.500000 ] [G loss : 20.693443]\n",
            "1790 [D loss : 0.500000 ] [G loss : 15.933593]\n",
            "1800 [D loss : 0.500000 ] [G loss : 12.433035]\n",
            "1810 [D loss : 0.500000 ] [G loss : 10.932922]\n",
            "1820 [D loss : 0.500000 ] [G loss : 14.569310]\n",
            "1830 [D loss : 0.500000 ] [G loss : 23.010502]\n",
            "1840 [D loss : 0.500000 ] [G loss : 20.469680]\n",
            "1850 [D loss : 0.500000 ] [G loss : 14.828039]\n",
            "1860 [D loss : 0.500000 ] [G loss : 10.206292]\n",
            "1870 [D loss : 0.500000 ] [G loss : 8.720369]\n",
            "1880 [D loss : 0.500000 ] [G loss : 11.716628]\n",
            "1890 [D loss : 0.500000 ] [G loss : 10.640231]\n",
            "1900 [D loss : 0.500000 ] [G loss : 16.083621]\n",
            "1910 [D loss : 0.500000 ] [G loss : 20.303832]\n",
            "1920 [D loss : 0.500000 ] [G loss : 9.292986]\n",
            "1930 [D loss : 0.500000 ] [G loss : 15.851014]\n",
            "1940 [D loss : 0.500000 ] [G loss : 10.888832]\n",
            "1950 [D loss : 0.500000 ] [G loss : 22.773699]\n",
            "1960 [D loss : 0.500000 ] [G loss : 8.317621]\n",
            "1970 [D loss : 0.500000 ] [G loss : 9.889824]\n",
            "1980 [D loss : 0.500000 ] [G loss : 16.667137]\n",
            "1990 [D loss : 0.500000 ] [G loss : 9.021382]\n",
            "2000 [D loss : 0.500000 ] [G loss : 9.169259]\n",
            "2010 [D loss : 0.500000 ] [G loss : 11.193483]\n",
            "2020 [D loss : 0.500000 ] [G loss : 8.371587]\n",
            "2030 [D loss : 0.500000 ] [G loss : 8.623148]\n",
            "2040 [D loss : 0.500000 ] [G loss : 39.249224]\n",
            "2050 [D loss : 0.500000 ] [G loss : 26.626429]\n",
            "2060 [D loss : 0.500000 ] [G loss : 8.510412]\n",
            "2070 [D loss : 0.500000 ] [G loss : 30.806378]\n",
            "2080 [D loss : 0.500000 ] [G loss : 20.525576]\n",
            "2090 [D loss : 0.500000 ] [G loss : 9.644046]\n",
            "2100 [D loss : 0.500000 ] [G loss : 16.038358]\n",
            "2110 [D loss : 0.500000 ] [G loss : 30.783064]\n",
            "2120 [D loss : 0.500000 ] [G loss : 10.969705]\n",
            "2130 [D loss : 0.500000 ] [G loss : 10.656618]\n",
            "2140 [D loss : 0.500000 ] [G loss : 26.802287]\n",
            "2150 [D loss : 0.500000 ] [G loss : 20.484668]\n",
            "2160 [D loss : 0.500000 ] [G loss : 20.123293]\n",
            "2170 [D loss : 0.500000 ] [G loss : 8.275841]\n",
            "2180 [D loss : 0.500000 ] [G loss : 9.813767]\n",
            "2190 [D loss : 0.500000 ] [G loss : 8.454864]\n",
            "2200 [D loss : 0.500000 ] [G loss : 22.797109]\n",
            "2210 [D loss : 0.500000 ] [G loss : 15.504397]\n",
            "2220 [D loss : 0.500000 ] [G loss : 21.169765]\n",
            "2230 [D loss : 0.500000 ] [G loss : 55.438845]\n",
            "2240 [D loss : 0.500000 ] [G loss : 18.242520]\n",
            "2250 [D loss : 0.500000 ] [G loss : 15.687270]\n",
            "2260 [D loss : 0.500000 ] [G loss : 19.768687]\n",
            "2270 [D loss : 0.500000 ] [G loss : 16.012768]\n",
            "2280 [D loss : 0.500000 ] [G loss : 31.814774]\n",
            "2290 [D loss : 0.500000 ] [G loss : 10.846957]\n",
            "2300 [D loss : 0.500000 ] [G loss : 9.069760]\n",
            "2310 [D loss : 0.500000 ] [G loss : 14.490143]\n",
            "2320 [D loss : 0.500000 ] [G loss : 11.437244]\n",
            "2330 [D loss : 0.500000 ] [G loss : 18.205102]\n",
            "2340 [D loss : 0.500000 ] [G loss : 8.072516]\n",
            "2350 [D loss : 0.500000 ] [G loss : 15.327428]\n",
            "2360 [D loss : 0.500000 ] [G loss : 15.022206]\n",
            "2370 [D loss : 0.500000 ] [G loss : 9.910756]\n",
            "2380 [D loss : 0.500000 ] [G loss : 10.028224]\n",
            "2390 [D loss : 0.500000 ] [G loss : 10.900282]\n",
            "2400 [D loss : 0.500000 ] [G loss : 9.572446]\n",
            "2410 [D loss : 0.500000 ] [G loss : 9.320446]\n",
            "2420 [D loss : 0.500000 ] [G loss : 10.767526]\n",
            "2430 [D loss : 0.500000 ] [G loss : 10.635733]\n",
            "2440 [D loss : 0.500000 ] [G loss : 32.142609]\n",
            "2450 [D loss : 0.500000 ] [G loss : 17.583541]\n",
            "2460 [D loss : 0.500000 ] [G loss : 18.711355]\n",
            "2470 [D loss : 0.500000 ] [G loss : 30.923770]\n",
            "2480 [D loss : 0.500000 ] [G loss : 15.821492]\n",
            "2490 [D loss : 0.500000 ] [G loss : 22.140167]\n",
            "2500 [D loss : 0.500000 ] [G loss : 14.635826]\n",
            "2510 [D loss : 0.500000 ] [G loss : 8.268484]\n",
            "2520 [D loss : 0.500000 ] [G loss : 9.843086]\n",
            "2530 [D loss : 0.500000 ] [G loss : 19.404201]\n",
            "2540 [D loss : 0.500000 ] [G loss : 16.689286]\n",
            "2550 [D loss : 0.500000 ] [G loss : 39.697851]\n",
            "2560 [D loss : 0.500000 ] [G loss : 28.907297]\n",
            "2570 [D loss : 0.500000 ] [G loss : 28.223302]\n",
            "2580 [D loss : 0.500000 ] [G loss : 9.116222]\n",
            "2590 [D loss : 0.500000 ] [G loss : 43.943382]\n",
            "2600 [D loss : 0.500000 ] [G loss : 23.058277]\n",
            "2610 [D loss : 0.500000 ] [G loss : 8.516706]\n",
            "2620 [D loss : 0.500000 ] [G loss : 8.617753]\n",
            "2630 [D loss : 0.500000 ] [G loss : 22.995176]\n",
            "2640 [D loss : 0.500000 ] [G loss : 39.698338]\n",
            "2650 [D loss : 0.500000 ] [G loss : 14.922918]\n",
            "2660 [D loss : 0.500000 ] [G loss : 18.490019]\n",
            "2670 [D loss : 0.500000 ] [G loss : 9.536239]\n",
            "2680 [D loss : 0.500000 ] [G loss : 55.053556]\n",
            "2690 [D loss : 0.500000 ] [G loss : 30.152548]\n",
            "2700 [D loss : 0.500000 ] [G loss : 35.591576]\n",
            "2710 [D loss : 0.500000 ] [G loss : 22.628032]\n",
            "2720 [D loss : 0.500000 ] [G loss : 39.763513]\n",
            "2730 [D loss : 0.500000 ] [G loss : 10.861607]\n",
            "2740 [D loss : 0.500000 ] [G loss : 41.708785]\n",
            "2750 [D loss : 0.500000 ] [G loss : 11.816227]\n",
            "2760 [D loss : 0.500000 ] [G loss : 37.083528]\n",
            "2770 [D loss : 0.500000 ] [G loss : 23.792421]\n",
            "2780 [D loss : 0.500000 ] [G loss : 9.810984]\n",
            "2790 [D loss : 0.500000 ] [G loss : 15.429878]\n",
            "2800 [D loss : 0.500000 ] [G loss : 11.054344]\n",
            "2810 [D loss : 0.500000 ] [G loss : 21.787835]\n",
            "2820 [D loss : 0.500000 ] [G loss : 8.498881]\n",
            "2830 [D loss : 0.500000 ] [G loss : 15.990647]\n",
            "2840 [D loss : 0.500000 ] [G loss : 33.450470]\n",
            "2850 [D loss : 0.500000 ] [G loss : 34.719465]\n",
            "2860 [D loss : 0.500000 ] [G loss : 31.175245]\n",
            "2870 [D loss : 0.500000 ] [G loss : 24.782786]\n",
            "2880 [D loss : 0.500000 ] [G loss : 15.692177]\n",
            "2890 [D loss : 0.500000 ] [G loss : 9.761268]\n",
            "2900 [D loss : 0.500000 ] [G loss : 11.880141]\n",
            "2910 [D loss : 0.500000 ] [G loss : 12.369740]\n",
            "2920 [D loss : 0.500000 ] [G loss : 15.780943]\n",
            "2930 [D loss : 0.500000 ] [G loss : 22.119874]\n",
            "2940 [D loss : 0.500000 ] [G loss : 45.071422]\n",
            "2950 [D loss : 0.500000 ] [G loss : 1367.688331]\n",
            "2960 [D loss : 0.500000 ] [G loss : 39.107387]\n",
            "2970 [D loss : 0.500000 ] [G loss : 22.094867]\n",
            "2980 [D loss : 0.500000 ] [G loss : 29.920443]\n",
            "2990 [D loss : 0.500000 ] [G loss : 30.740193]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OFJn4tGnnsM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4982a476-c088-4385-abe2-214740009d83"
      },
      "source": [
        "A = read_image('/content/drive/MyDrive/images/10000.jpg')\n",
        "A = np.expand_dims(A,axis=0)\n",
        "res = gan.g_AB.predict(A)\n",
        "print(da['10000'])\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 3, 25, 128, 16, 4, 1]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}