{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conditional_GAN_auto_label_encoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RVijayChiranjithReddy/Autolabel_encoder/blob/main/conditional_GAN_auto_label_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjegNPOY79Pf",
        "outputId": "cce96f03-2740-474f-deb0-54b31826b149"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import *\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.activations import *\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.applications import *\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import glob\n",
        "from random import randint, shuffle\n",
        "%matplotlib inline\n",
        "K.set_learning_phase(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgcggMYdcq4s",
        "outputId": "3fc3a363-1be5-44c1-c34a-27c361419684"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cauWny0dAUR",
        "outputId": "6857abc1-2ffd-434a-be0e-5977d45c62ab"
      },
      "source": [
        "import pandas\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "df = pandas.read_csv('/content/styles.csv')\n",
        "df.fillna('DK', inplace = True) \n",
        "data = {}\n",
        "for i in range(44446):\n",
        "  tags = [str(x) for x in df.loc[i].values[1:9] if type(x) is str]\n",
        "  #desc = df.loc[i].values[9]\n",
        "  #if type(desc) is not float:\n",
        "    #for de in desc.split():\n",
        "     #if de.isalpha() and de not in tags:\n",
        "        #tags.append(de)\n",
        "  data[str(df.loc[i].values[0])] = ':'.join(tags)\n",
        "vocab = []\n",
        "for k,v in data.items():\n",
        "  temp = list(set(v.split(':')))\n",
        "  vocab = vocab + temp\n",
        "  vocab = list(set(vocab))\n",
        "print(len(vocab))\n",
        "#from tensorflow.keras.preprocessing.text import one_hot\n",
        "#for k,d in data.items():\n",
        "  #data_encoded[k] = one_hot(' '.join(d),len(vocab))\n",
        "tokenizer1 = Tokenizer(num_words=25)\n",
        "tokenizer2 = Tokenizer(num_words=25)\n",
        "tokenizer3 = Tokenizer(num_words=54)\n",
        "tokenizer4 = Tokenizer(num_words=173)\n",
        "tokenizer5 = Tokenizer(num_words=48)\n",
        "tokenizer6 = Tokenizer(num_words=25)\n",
        "tokenizer7 = Tokenizer(num_words=25)\n",
        "# This builds the word index\n",
        "tokenizer1.fit_on_texts(df['gender'])\n",
        "tokenizer2.fit_on_texts(df['masterCategory'])\n",
        "tokenizer3.fit_on_texts(df['subCategory'])\n",
        "tokenizer4.fit_on_texts(df['articleType'])\n",
        "tokenizer5.fit_on_texts(df['baseColour'])\n",
        "tokenizer6.fit_on_texts(df['season'])\n",
        "tokenizer7.fit_on_texts(df['usage'])\n",
        "\n",
        "\n",
        "# This turns strings into lists of integer indices.\n",
        "#train_sequences = tokenizer.texts_to_sequences(data.values())\n",
        "print(tokenizer1.word_index)\n",
        "print(tokenizer2.word_index)\n",
        "print(tokenizer3.word_index)\n",
        "print(tokenizer4.word_index)\n",
        "print(tokenizer5.word_index)\n",
        "print(tokenizer6.word_index)\n",
        "print(tokenizer7.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "240\n",
            "{'men': 1, 'women': 2, 'unisex': 3, 'boys': 4, 'girls': 5}\n",
            "{'apparel': 1, 'accessories': 2, 'footwear': 3, 'personal': 4, 'care': 5, 'free': 6, 'items': 7, 'sporting': 8, 'goods': 9, 'home': 10}\n",
            "{'topwear': 1, 'shoes': 2, 'bags': 3, 'bottomwear': 4, 'watches': 5, 'innerwear': 6, 'jewellery': 7, 'eyewear': 8, 'fragrance': 9, 'sandal': 10, 'wallets': 11, 'flip': 12, 'flops': 13, 'belts': 14, 'socks': 15, 'lips': 16, 'and': 17, 'dress': 18, 'loungewear': 19, 'nightwear': 20, 'saree': 21, 'nails': 22, 'makeup': 23, 'headwear': 24, 'ties': 25, 'accessories': 26, 'skin': 27, 'scarves': 28, 'cufflinks': 29, 'apparel': 30, 'set': 31, 'free': 32, 'gifts': 33, 'stoles': 34, 'care': 35, 'eyes': 36, 'mufflers': 37, 'shoe': 38, 'sports': 39, 'equipment': 40, 'gloves': 41, 'hair': 42, 'bath': 43, 'body': 44, 'water': 45, 'bottle': 46, 'perfumes': 47, 'umbrellas': 48, 'beauty': 49, 'wristbands': 50, 'home': 51, 'furnishing': 52, 'vouchers': 53}\n",
            "{'tshirts': 1, 'shoes': 2, 'shirts': 3, 'casual': 4, 'watches': 5, 'sports': 6, 'kurtas': 7, 'tops': 8, 'handbags': 9, 'heels': 10, 'sunglasses': 11, 'and': 12, 'sandals': 13, 'wallets': 14, 'flip': 15, 'flops': 16, 'briefs': 17, 'belts': 18, 'backpacks': 19, 'socks': 20, 'formal': 21, 'body': 22, 'perfume': 23, 'mist': 24, 'jeans': 25, 'shorts': 26, 'trousers': 27, 'flats': 28, 'bra': 29, 'dresses': 30, 'sarees': 31, 'earrings': 32, 'pants': 33, 'deodorant': 34, 'nail': 35, 'polish': 36, 'lipstick': 37, 'track': 38, 'clutches': 39, 'sweatshirts': 40, 'caps': 41, 'sweaters': 42, 'ties': 43, 'jackets': 44, 'innerwear': 45, 'vests': 46, 'set': 47, 'kurtis': 48, 'tunics': 49, 'bag': 50, 'lip': 51, 'nightdress': 52, 'leggings': 53, 'pendant': 54, 'capris': 55, 'gift': 56, 'necklace': 57, 'chains': 58, 'gloss': 59, 'suits': 60, 'night': 61, 'trunk': 62, 'accessory': 63, 'skirts': 64, 'dupatta': 65, 'scarves': 66, 'ring': 67, 'cufflinks': 68, 'kajal': 69, 'eyeliner': 70, 'lounge': 71, 'face': 72, 'kurta': 73, 'sets': 74, 'free': 75, 'gifts': 76, 'stoles': 77, 'duffel': 78, 'bangle': 79, 'laptop': 80, 'foundation': 81, 'primer': 82, 'bracelet': 83, 'pouch': 84, 'moisturisers': 85, 'jewellery': 86, 'fragrance': 87, 'highlighter': 88, 'blush': 89, 'boxers': 90, 'compact': 91, 'liner': 92, 'mobile': 93, 'messenger': 94, 'eyeshadow': 95, 'suspenders': 96, 'camisoles': 97, 'salwar': 98, 'mufflers': 99, 'patiala': 100, 'jeggings': 101, 'stockings': 102, 'churidar': 103, 'wash': 104, 'tracksuits': 105, 'cleanser': 106, 'sunscreen': 107, 'shoe': 108, 'robe': 109, 'accessories': 110, 'rain': 111, 'bath': 112, 'gloves': 113, 'hair': 114, 'colour': 115, 'jacket': 116, 'swimwear': 117, 'waist': 118, 'baby': 119, 'dolls': 120, 'travel': 121, 'care': 122, 'jumpsuit': 123, 'waistcoat': 124, 'basketballs': 125, 'mascara': 126, 'mask': 127, 'peel': 128, 'rompers': 129, 'booties': 130, 'water': 131, 'bottle': 132, 'concealer': 133, 'rucksacks': 134, 'tights': 135, 'shapewear': 136, 'blazers': 137, 'footballs': 138, 'clothing': 139, 'headband': 140, 'wristbands': 141, 'shrug': 142, 'eye': 143, 'cream': 144, 'essentials': 145, 'scrub': 146, 'lotion': 147, 'umbrellas': 148, 'exfoliator': 149, 'nehru': 150, 'toner': 151, 'beauty': 152, 'lehenga': 153, 'choli': 154, 'makeup': 155, 'remover': 156, 'plumper': 157, 'trolley': 158, 'tablet': 159, 'sleeve': 160, 'hat': 161, 'key': 162, 'chain': 163, 'serum': 164, 'gel': 165, 'laces': 166, 'cushion': 167, 'covers': 168, 'mens': 169, 'grooming': 170, 'kit': 171, 'ipad': 172}\n",
            "{'black': 1, 'blue': 2, 'white': 3, 'brown': 4, 'grey': 5, 'red': 6, 'green': 7, 'pink': 8, 'navy': 9, 'purple': 10, 'silver': 11, 'yellow': 12, 'beige': 13, 'gold': 14, 'maroon': 15, 'orange': 16, 'olive': 17, 'multi': 18, 'cream': 19, 'steel': 20, 'charcoal': 21, 'peach': 22, 'off': 23, 'skin': 24, 'lavender': 25, 'melange': 26, 'khaki': 27, 'magenta': 28, 'teal': 29, 'tan': 30, 'mustard': 31, 'bronze': 32, 'copper': 33, 'turquoise': 34, 'rust': 35, 'burgundy': 36, 'metallic': 37, 'coffee': 38, 'mauve': 39, 'rose': 40, 'nude': 41, 'sea': 42, 'mushroom': 43, 'dk': 44, 'taupe': 45, 'lime': 46, 'fluorescent': 47}\n",
            "{'summer': 1, 'fall': 2, 'winter': 3, 'spring': 4, 'dk': 5}\n",
            "{'casual': 1, 'sports': 2, 'ethnic': 3, 'formal': 4, 'dk': 5, 'smart': 6, 'party': 7, 'travel': 8, 'home': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk8p7t_eWP3j",
        "outputId": "ff538eb8-84d6-461a-aa83-a0948872c392"
      },
      "source": [
        "print(data['48311'].split(':')[4])\n",
        "print(tokenizer5.texts_to_sequences([['bronze']]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bronze\n",
            "[[32]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLDz0_MMdHM_"
      },
      "source": [
        "da = {}\n",
        "ds = {}\n",
        "for k,v in data.items():\n",
        "  da[str(k)] =[]\n",
        "  da[str(k)].append(tokenizer1.texts_to_sequences([v.split(':')[0]])[0][0])\n",
        "  da[str(k)].append(tokenizer2.texts_to_sequences([v.split(':')[1]])[0][0])\n",
        "  da[str(k)].append(tokenizer3.texts_to_sequences([v.split(':')[2]])[0][0])\n",
        "  da[str(k)].append(tokenizer4.texts_to_sequences([v.split(':')[3]])[0][0])\n",
        "  da[str(k)].append(tokenizer5.texts_to_sequences([v.split(':')[4]])[0][0])\n",
        "  da[str(k)].append(tokenizer6.texts_to_sequences([v.split(':')[5]])[0][0])\n",
        "  da[str(k)].append(tokenizer7.texts_to_sequences([v.split(':')[6]])[0][0])\n",
        "  i=i+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpalvD92dPHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e287af06-bcc8-4e0e-ce6b-646c0d4c5ea7"
      },
      "source": [
        "def generator(img_shape):\n",
        "  inputs = Input(shape= img_shape)\n",
        "  x  = Conv2D(64,padding = 'same',strides = 4,kernel_size = 3)(inputs)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  x1 = MaxPooling2D((3,3),strides =(1,1),padding='same')(x)\n",
        "  x1  = Conv2D(64,padding = 'same',strides = 1,kernel_size = 1)(x1)\n",
        "  x1 = BatchNormalization()(x1)\n",
        "  x1 = LeakyReLU(alpha=0.2)(x1)\n",
        "  x3  = Conv2D(64,padding = 'same',strides = 1,kernel_size = 1)(x)\n",
        "  x3  = Conv2D(64,padding = 'same',strides = 1,kernel_size = 3)(x3)\n",
        "  x3 = BatchNormalization()(x3)\n",
        "  x3 = LeakyReLU(alpha=0.2)(x3)\n",
        "  x5  = Conv2D(64,padding = 'same',strides = 1,kernel_size = 1)(x)\n",
        "  x5  = Conv2D(64,padding = 'same',strides = 1,kernel_size = 5)(x5)\n",
        "  x5 = BatchNormalization()(x5)\n",
        "  x5 = LeakyReLU(alpha=0.2)(x5)\n",
        "  x_135 = concatenate([x,x1,x3,x5],axis = -1)\n",
        "  x21  = Conv2D(32,padding = 'same',strides = 2,kernel_size = 3)(x_135)\n",
        "  x21 = BatchNormalization()(x21)\n",
        "  x21 = LeakyReLU(alpha=0.2)(x21)\n",
        "  x22 = MaxPooling2D((3,3),strides =(1,1),padding='same')(x21)\n",
        "  x22  = Conv2D(32,padding = 'same',strides = 1,kernel_size = 1)(x22)\n",
        "  x22 = BatchNormalization()(x22)\n",
        "  x22 = LeakyReLU(alpha=0.2)(x22)\n",
        "  x23  = Conv2D(32,padding = 'same',strides =1,kernel_size = 1)(x21)\n",
        "  x23  = Conv2D(32,padding = 'same',strides = 1,kernel_size = 3)(x23)\n",
        "  x23 = BatchNormalization()(x23)\n",
        "  x23 = LeakyReLU(alpha=0.2)(x23)\n",
        "  x25  = Conv2D(32,padding = 'same',strides = 1,kernel_size = 1)(x21)\n",
        "  x25  = Conv2D(32,padding = 'same',strides = 1,kernel_size = 3)(x25)\n",
        "  x25 = BatchNormalization()(x25)\n",
        "  x25 = LeakyReLU(alpha=0.2)(x25)\n",
        "  x_2135 = concatenate([x21,x22,x23,x25],axis=-1)\n",
        "  x  = Conv2D(64,padding = 'same',strides = 2,kernel_size = 3)(x_2135)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  x  = Conv2D(32,padding = 'same',strides = 2,kernel_size = 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  x  = Conv2D(16,padding = 'same',strides = 2,kernel_size = 3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(4096,activation = 'relu')(x)\n",
        "  x = Dense(1024,activation = 'relu')(x)\n",
        "  x = Dense(7,activation = 'relu')(x)\n",
        "  return Model(inputs= inputs,outputs = x)\n",
        "g = generator((256,256,3))\n",
        "g.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 64, 64, 64)   1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 64, 64, 64)   256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 64, 64, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 64, 64, 64)   0           leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 64, 64, 64)   4160        leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 64)   4160        leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 64, 64, 64)   4160        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 64, 64, 64)   36928       conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 64)   102464      conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 64, 64, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 64, 64, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 64, 64, 256)  0           leaky_re_lu[0][0]                \n",
            "                                                                 leaky_re_lu_1[0][0]              \n",
            "                                                                 leaky_re_lu_2[0][0]              \n",
            "                                                                 leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 32)   73760       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 32)   1056        leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 32)   1056        leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 32)   1056        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 32)   9248        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 32)   128         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 32, 32, 32)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 32, 32, 32)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 128)  0           leaky_re_lu_4[0][0]              \n",
            "                                                                 leaky_re_lu_5[0][0]              \n",
            "                                                                 leaky_re_lu_6[0][0]              \n",
            "                                                                 leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 64)   73792       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 32)     18464       leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 8, 8, 32)     128         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 8, 8, 32)     0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 4, 4, 16)     4624        leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 4, 4, 16)     64          conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 4, 4, 16)     0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 256)          0           leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 4096)         1052672     flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         4195328     dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 7)            7175        dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,603,127\n",
            "Trainable params: 5,602,135\n",
            "Non-trainable params: 992\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXsQQNwWdVYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836ae4f9-eb6f-4913-ede3-2f50d8244091"
      },
      "source": [
        "batch_size = 64\n",
        "def cgan_gen():\n",
        "  input1 = Input(shape= (256,256,3))\n",
        "  input2 = Input(shape = (7,))\n",
        "  i = Embedding(input_dim=239,output_dim=8,input_length=13)(input2)\n",
        "  x = Flatten()(i)\n",
        "  gener = generator(img_shape=(256,256,3))\n",
        "  x1 = gener(input1)\n",
        "  lt = Dense(56,activation='relu')(x1)\n",
        "  x = Add()([lt,x])\n",
        "  x = Dense(4096,activation = 'relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(1024,activation = 'relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(239,activation = 'relu')(x)\n",
        "  x = Dense(7,activation = 'relu')(x)\n",
        "  return Model(inputs = [input1,input2],outputs = x)\n",
        "c = cgan_gen()\n",
        "c.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 7)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Functional)            (None, 7)            5603127     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 7, 8)         1912        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 56)           448         model_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 56)           0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 56)           0           dense_6[0][0]                    \n",
            "                                                                 flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 4096)         233472      add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 4096)         16384       dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1024)         4195328     batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 1024)         4096        dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 239)          244975      batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 7)            1680        dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 10,301,422\n",
            "Trainable params: 10,290,190\n",
            "Non-trainable params: 11,232\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGDuOprPdaoP"
      },
      "source": [
        "def discriminator():\n",
        "  x1 = Input(shape = (256,256,3))\n",
        "  x2 = Input(shape=(7,))\n",
        "  discr = generator(img_shape=(256,256,3))\n",
        "  xn = discr(x1)\n",
        "  xn = Dense(56,activation = 'relu')(xn)\n",
        "  i = Embedding(input_dim=len(vocab),output_dim=8,input_length=7)(x2)\n",
        "  x = Flatten()(i)\n",
        "  xc = concatenate([xn,x])\n",
        "  x = Dense(4096,activation='relu')(xc)\n",
        "  x = BatchNormalization(momentum=0.9)(x)\n",
        "  x = Dense(4096,activation='relu')(x)\n",
        "  x= BatchNormalization(momentum=0.9)(x)\n",
        "  x = Dense(1,activation='sigmoid')(x)\n",
        "  return Model(inputs = [x1,x2],outputs = x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JaDNBAqddsE",
        "outputId": "c724c074-ae3b-45a3-e913-0fff717a455a"
      },
      "source": [
        "def gan(disc,cgan):\n",
        "  x1 = Input(shape = (256,256,3))\n",
        "  x2 = Input(shape = (7,))\n",
        "  xc = cgan([x1,x2]) \n",
        "  x = disc([x1,xc])\n",
        "  return Model([x1,x2],x)\n",
        "disc = discriminator()\n",
        "disc.compile(optimizer = Adam(lr=0.001),loss ='binary_crossentropy',metrics=['accuracy'])\n",
        "cgan = cgan_gen()\n",
        "#cgan.compile(optimizer = Adam(lr=0.001),loss ='categorical_crossentropy',metrics=['accuracy'])\n",
        "disc.trainable = False\n",
        "GAN = gan(disc,cgan)\n",
        "GAN.compile(loss = 'binary_crossentropy',optimizer = Adam(lr=0.001),metrics=['accuracy'])\n",
        "GAN.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           [(None, 7)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_6 (Functional)            (None, 7)            10301422    input_11[0][0]                   \n",
            "                                                                 input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "model_4 (Functional)            (None, 1)            22886520    input_11[0][0]                   \n",
            "                                                                 model_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 33,187,942\n",
            "Trainable params: 10,290,190\n",
            "Non-trainable params: 22,897,752\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-zfBqDUdhJt"
      },
      "source": [
        "def read_image(fn):\n",
        "  im = cv2.imread(fn)\n",
        "  im = cv2.cvtColor(im, cv2.COLOR_BGR2LAB)\n",
        "  im = cv2.resize(im, (256,256), interpolation=cv2.INTER_CUBIC)\n",
        "  im = np.array(im)/255*2-1\n",
        "  return im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buZd7QkDdjzV"
      },
      "source": [
        "traini = glob.glob(r'/content/drive/MyDrive/images/*')\n",
        "def minibatch(data, batchsize):\n",
        "    length = len(data)\n",
        "    epoch = i = 0\n",
        "    tmpsize = None    \n",
        "    while True:\n",
        "        size = tmpsize if tmpsize else batchsize\n",
        "        if i+size > length:\n",
        "            shuffle(data)\n",
        "            i = 0\n",
        "            epoch+=1        \n",
        "        rtn = [read_image(data[j]) for j in range(i,i+size)]\n",
        "        ltn=[]\n",
        "        se = []\n",
        "        for j in range(i,i+size):\n",
        "          if '(' in data[j].split('/')[-1].split('.')[0]:\n",
        "            t=[]\n",
        "            p = data[j].split('/')[-1].split('.')[0].split(' ')[0]\n",
        "            t.append(da[p][0]/5)\n",
        "            t.append(da[p][1]/10)\n",
        "            t.append(da[p][2]/53)\n",
        "            t.append(da[p][3]/172)\n",
        "            t.append(da[p][4]/47)\n",
        "            t.append(da[p][5]/5)\n",
        "            t.append(da[p][6]/9)\n",
        "            se.append(t) \n",
        "          else:\n",
        "            t=[]\n",
        "            p = data[j].split('/')[-1].split('.')[0]\n",
        "            t.append(da[p][0]/5)\n",
        "            t.append(da[p][1]/10)\n",
        "            t.append(da[p][2]/53)\n",
        "            t.append(da[p][3]/172)\n",
        "            t.append(da[p][4]/47)\n",
        "            t.append(da[p][5]/5)\n",
        "            t.append(da[p][6]/9)\n",
        "            se.append(t) \n",
        "        #ltn = [data_enc_padded[data[j].split('/')[-1].split('.')[0]] for j in range(i,i+size)]\n",
        "        i+=size\n",
        "        tmpsize = yield epoch, np.float32(rtn), np.array(se)\n",
        "\n",
        "def minibatchAB(dataA, batchsize):\n",
        "    batchA = minibatch(dataA, batchsize)\n",
        "    tmpsize = None    \n",
        "    while True:        \n",
        "        ep1, A ,B = batchA.send(tmpsize)\n",
        "        tmpsize = yield ep1, A , B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sroQ-cj9dku_"
      },
      "source": [
        "def train(iterations, batch_size, sample_interval):\n",
        "  train_batch = minibatchAB(traini,128)\n",
        "  for iteration in range(iterations):\n",
        "    for i in range(1):\n",
        "      epoch, A,B= next(train_batch)\n",
        "      z = np.random.rand(batch_size, 7)\n",
        "      f = cgan.predict([A,z])\n",
        "      d_loss_fake = disc.train_on_batch([A,f],np.zeros(shape=(batch_size,1)))\n",
        "      d_loss_real = disc.train_on_batch([A,B],np.ones(shape=(batch_size,1)))\n",
        "      print('hi')   \n",
        "    d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "    epoch, A,B = next(train_batch)\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
        "    #g_loss = cgan.train_on_batch([A,z],S)\n",
        "    #g_loss = GAN.train_on_batch([A,z],np.ones(shape=(batch_size,1))) \n",
        "    with tf.GradientTape() as tape:\n",
        "      z = np.random.rand(batch_size, 7)\n",
        "      y = GAN([A,z])\n",
        "      loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(np.ones(shape=(batch_size,1)),y))\n",
        "      varis = cgan.trainable_variables + disc.variables\n",
        "      grads = tape.gradient(loss, varis)\n",
        "      optimizer.apply_gradients(zip(grads,varis))\n",
        "    if (iteration + 1) % sample_interval == 0:\n",
        "      print(\"%d [D loss: %f, acc.: %.2f%%]\" % (iteration + 1, np.sum(d_loss), 100.0 * np.sum(accuracy)))\n",
        "      print(tf.reduce_sum(loss))\n",
        "      cgan.save('/content/drive/MyDrive/cgan.h5')\n",
        "      disc.save('/content/drive/MyDrive/disc.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrN5n5CFf3pT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0effa3b-bcad-4543-f335-6c8d865297b6"
      },
      "source": [
        "train(2000,128,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "1 [D loss: 0.810956, acc.: 49.61%]\n",
            "tf.Tensor(8.6700677e-13, shape=(), dtype=float32)\n",
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "2 [D loss: 0.723553, acc.: 50.00%]\n",
            "tf.Tensor(8.624272e-07, shape=(), dtype=float32)\n",
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "3 [D loss: 0.763641, acc.: 48.83%]\n",
            "tf.Tensor(5.5028574e-24, shape=(), dtype=float32)\n",
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "4 [D loss: 0.756183, acc.: 50.00%]\n",
            "tf.Tensor(6.6758535e-20, shape=(), dtype=float32)\n",
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "5 [D loss: 0.710956, acc.: 50.00%]\n",
            "tf.Tensor(5.454365e-13, shape=(), dtype=float32)\n",
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "6 [D loss: 1.005713, acc.: 1.17%]\n",
            "tf.Tensor(4.2613303e-13, shape=(), dtype=float32)\n",
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "7 [D loss: 0.961613, acc.: 1.17%]\n",
            "tf.Tensor(7.102851e-09, shape=(), dtype=float32)\n",
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "8 [D loss: 0.732352, acc.: 50.00%]\n",
            "tf.Tensor(2.3370044e-19, shape=(), dtype=float32)\n",
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "9 [D loss: 0.701539, acc.: 50.00%]\n",
            "tf.Tensor(3.906788e-18, shape=(), dtype=float32)\n",
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "10 [D loss: 0.700628, acc.: 50.00%]\n",
            "tf.Tensor(4.556513e-13, shape=(), dtype=float32)\n",
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "11 [D loss: 0.699801, acc.: 50.00%]\n",
            "tf.Tensor(0.34173954, shape=(), dtype=float32)\n",
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "12 [D loss: 0.795090, acc.: 50.00%]\n",
            "tf.Tensor(1044.6482, shape=(), dtype=float32)\n",
            "hi\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_45/kernel:0', 'conv2d_45/bias:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_59/kernel:0', 'conv2d_59/bias:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'embedding_2/embeddings:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_23/kernel:0', 'dense_23/bias:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'batch_normalization_24/moving_mean:0', 'batch_normalization_24/moving_variance:0', 'batch_normalization_25/moving_mean:0', 'batch_normalization_25/moving_variance:0', 'batch_normalization_26/moving_mean:0', 'batch_normalization_26/moving_variance:0', 'batch_normalization_27/moving_mean:0', 'batch_normalization_27/moving_variance:0', 'batch_normalization_28/moving_mean:0', 'batch_normalization_28/moving_variance:0', 'batch_normalization_29/moving_mean:0', 'batch_normalization_29/moving_variance:0', 'batch_normalization_30/moving_mean:0', 'batch_normalization_30/moving_variance:0', 'batch_normalization_31/moving_mean:0', 'batch_normalization_31/moving_variance:0', 'batch_normalization_32/moving_mean:0', 'batch_normalization_32/moving_variance:0', 'batch_normalization_33/moving_mean:0', 'batch_normalization_33/moving_variance:0', 'batch_normalization_34/moving_mean:0', 'batch_normalization_34/moving_variance:0', 'batch_normalization_35/moving_mean:0', 'batch_normalization_35/moving_variance:0', 'batch_normalization_36/moving_mean:0', 'batch_normalization_36/moving_variance:0'] when minimizing the loss.\n",
            "13 [D loss: 1.277711, acc.: 50.00%]\n",
            "tf.Tensor(0.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-1038d9507d7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-1e1cae0ddadd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(iterations, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m       \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-1a5ae73784c7>\u001b[0m in \u001b[0;36mminibatchAB\u001b[0;34m(dataA, batchsize)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtmpsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mep1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mtmpsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mep1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-1a5ae73784c7>\u001b[0m in \u001b[0;36mminibatch\u001b[0;34m(data, batchsize)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mepoch\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mrtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mltn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-1a5ae73784c7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mepoch\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mrtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mltn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-958b8fe63d87>\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2LAB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBGjsbKQNTgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfecce7-15ea-43e3-b80c-02f4b0b500d2"
      },
      "source": [
        "z = np.random.rand(1, 7)\n",
        "img = read_image('/content/drive/MyDrive/images/9475.jpg')\n",
        "img = np.expand_dims(img,axis = 0)\n",
        "print(z.shape)\n",
        "a = cgan.predict([img,z])\n",
        "print(a)\n",
        "print(da['9475'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 7)\n",
            "[[0.00118032 0.00136031 0.         0.         0.00567467 0.\n",
            "  0.        ]]\n",
            "[1, 1, 1, 3, 2, 2, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}